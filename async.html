<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Arnaud Calmettes (nohar)">
  <title>'N' exemples pour découvrir la programmation asynchrone</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" href="default.css">
  
  
</head>
<body>
<header>
<h1 class="title">'N' exemples pour découvrir la programmation asynchrone</h1>
<h2 class="author">Arnaud Calmettes (nohar)</h2>
</header>
<h1 id="ça-veut-dire-quoi-asynchrone"><span class="header-section-number">1</span> Ça veut dire quoi, <em>asynchrone</em> ?</h1>
<p>En un mot comme en cent, un programme qui fonctionne de façon <em>asychrone</em>, c'est un programme qui évite au maximum de passer du temps à <em>attendre sans rien faire</em>, et qui s'arrange pour <em>s'occuper autant que possible pendant qu'il attend</em>. Cette façon d'optimiser le temps d'attente est tout à fait naturelle pour nous. Par exemple, on peut s'en rendre compte en observant le travail d'un serveur qui monte votre commande dans un <em>fast food</em>.</p>
<p>De façon synchrone :</p>
<ul>
<li>Préparer le hamburger :
<ul>
<li>Demander le hamburger en cuisine.</li>
<li>Attendre le hamburger (1 minute).</li>
<li>Récupérer le hamburger et le poser sur le plateau.</li>
</ul></li>
<li>Préparer les frites :
<ul>
<li>Mettre des frites à chauffer.</li>
<li>Attendre que les frites soient cuites (2 minutes).</li>
<li>Récupérer des frites et les poser sur le plateau.</li>
</ul></li>
<li>Préparer la boisson :
<ul>
<li>Placer un gobelet dans la machine à soda.</li>
<li>Remplir le gobelet (30 secondes).</li>
<li>Récupérer le gobelet et le poser sur le plateau.</li>
</ul></li>
</ul>
<p>En gros, si notre employé de <em>fast food</em> était synchrone, il mettrait 3 minutes et 30 secondes pour monter votre commande.</p>
<p>Alors que de façon asynchrone :</p>
<ul>
<li>Demander le hamburger en cuisine.</li>
<li>Mettre les frites à chauffer.</li>
<li>Placer un gobelet dans la machine à soda et le mettre à remplir.</li>
<li>Après 30 secondes: Récupérer le gobelet et le poser sur le plateau.</li>
<li>Après 1 minute : Récupérer le hamburger et le poser sur le plateau.</li>
<li>Après 2 minutes : Récupérer les frites et les poser sur le plateau.</li>
</ul>
<p>En travaillant de façon asynchrone, notre employé de <em>fast food</em> monte maintenant votre commande en 2 minutes. Mais ça ne s'arrête pas là !</p>
<ul>
<li><strong>Une commande <code>A</code> est confiée à l'employé</strong></li>
<li>Demander le burger pour <code>A</code> en cuisine</li>
<li>Mettre les frites à chauffer.</li>
<li>Placer un gobelet dans la machine à soda pour <code>A</code>.</li>
<li>Après 30 secondes : Récupérer le gobelet de <code>A</code> et le poser sur son plateau</li>
<li><strong>Une nouvelle commande <code>B</code> est prise et confiée à l'employé</strong></li>
<li>Demander le burger pour <code>B</code> en cuisine</li>
<li>Placer un gobelet dans la machine à soda pour <code>B</code>.</li>
<li>Après 1 minute : Le burger de <code>A</code> est prêt, le poser sur son plateau.</li>
<li>La boisson de <code>B</code> est remplie, la poser sur son plateau.</li>
<li>Après 1 minute 40 : Le burger de <code>B</code> est prêt, le poser sur son plateau.</li>
<li>Après 2 minutes : Les frites sont prêtes, servir <code>A</code> et <code>B</code></li>
</ul>
<p>Toujours en 2 minutes, l'employé asynchrone vient cette fois de servir 2 clients. Si vous vous mettez à la place du client <code>B</code> qui aurait dû attendre que l'employé finisse de monter la commande de <code>A</code> avant de s'occuper de la sienne dans un schéma synchrone, celui-ci a été servi en 1 minute 30 au lieu d'attendre 6 minutes 30.</p>
<p>Pensez-y la prochaine fois que vous irez manger dans un fast-food, et observez les serveurs. Leur boulot vous semblera d'un coup beaucoup plus compliqué qu'il n'y paraît !</p>
<p>En informatique, il existe un type de tâche qui impose aux programmes d'attendre sans rien faire : ce sont les <em>entrées/sorties</em> (ou <em>IO</em>). Nous verrons dans cet article que la programmation asynchrone est une façon extrêmement puissante d'implémenter des programmes qui réalisent plus d'IO que de calcul (comme une application de messagerie instantanée, par exemple).</p>
<h1 id="exemple-n1-une-boucle-événementielle-cest-essentiel"><span class="header-section-number">2</span> Exemple n°1 : Une boucle événementielle, c'est essentiel</h1>
<p>La notion fondamentale autour de laquelle <code>asyncio</code> a été construite est celle de <em>coroutine</em>.</p>
<p>Une coroutine est une tâche qui peut décider de se suspendre elle-même au moyen du mot-clé <code>yield</code>, et attendre jusqu'à ce que le code qui la contrôle décide de lui rendre la main en <em>itérant</em> dessus.</p>
<p>On peut imaginer, par exemple, écrire la fonction suivante :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> tic_tac():
    <span class="dt">print</span>(<span class="st">&quot;Tic&quot;</span>)
    <span class="kw">yield</span>
    <span class="dt">print</span>(<span class="st">&quot;Tac&quot;</span>)
    <span class="kw">yield</span>
    <span class="kw">return</span> <span class="st">&quot;Boum!&quot;</span></code></pre>
<p>Cette fonction, puisqu'elle utilise le mot-clé <code>yield</code>, définit une <em>coroutine</em><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Si on l'invoque, la fonction <code>tic_tac</code> retourne une tâche prête à être exécutée, mais n'exécute pas les instructions qu'elle contient.</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; task = tic_tac()
&gt;&gt;&gt; task
&lt;generator <span class="dt">object</span> tic_tac at <span class="bn">0x7fe157023280</span>&gt;</code></pre>
<p>En termes de vocabulaire, on dira que notre fonction <code>tic_tac</code> est une <em>fonction coroutine</em>, c'est-à-dire une fonction qui <strong>construit une coroutine</strong>. La coroutine est contenue ici dans la variable <code>task</code>.</p>
<p>Nous pouvons maintenant exécuter son code jusqu'au prochain <code>yield</code>, en nous servant de la fonction standard <code>next()</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="dt">next</span>(task)
Tic
&gt;&gt;&gt; <span class="dt">next</span>(task)
Tac
&gt;&gt;&gt; <span class="dt">next</span>(task)
Traceback (most recent call last):
  File <span class="st">&quot;&lt;stdin&gt;&quot;</span>, line <span class="dv">1</span>, in &lt;module&gt;
<span class="ot">StopIteration</span>: Boum!</code></pre>
<p>Lorsque la tâche est terminée, une exception <code>StopIteration</code> est levée. Celle-ci contient la valeur de retour de la coroutine. Jusqu'ici, rien de bien sorcier. Dès lors, on peut imaginer créer une petite boucle pour exécuter cette coroutine jusqu'à épuisement :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; task = tic_tac()
&gt;&gt;&gt; <span class="kw">while</span> <span class="ot">True</span>:
...     <span class="kw">try</span>:
...         <span class="dt">next</span>(task)
...     <span class="kw">except</span> <span class="ot">StopIteration</span> <span class="ch">as</span> stop:
...         <span class="dt">print</span>(<span class="st">&quot;valeur de retour:&quot;</span>, <span class="dt">repr</span>(stop.value))
...         <span class="kw">break</span>
...
Tic
Tac
valeur de retour: <span class="st">&#39;Boum!&#39;</span></code></pre>
<p>Afin de nous affranchir de la sémantique des itérateurs de Python, créons une classe <code>Task</code> qui nous permettra de manipuler nos coroutines plus aisément :</p>
<pre class="sourceCode python"><code class="sourceCode python">STATUS_NEW = <span class="st">&#39;NEW&#39;</span>
STATUS_RUNNING = <span class="st">&#39;RUNNING&#39;</span>
STATUS_FINISHED = <span class="st">&#39;FINISHED&#39;</span>
STATUS_ERROR = <span class="st">&#39;ERROR&#39;</span>

<span class="kw">class</span> Task:
    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, coro):
        <span class="ot">self</span>.coro = coro  <span class="co"># Coroutine à exécuter</span>
        <span class="ot">self</span>.name = coro.<span class="ot">__name__</span>
        <span class="ot">self</span>.status = STATUS_NEW  <span class="co"># Statut de la tâche</span>
        <span class="ot">self</span>.return_value = <span class="ot">None</span>  <span class="co"># Valeur de retour de la coroutine</span>
        <span class="ot">self</span>.error_value = <span class="ot">None</span>  <span class="co"># Exception levée par la coroutine</span>

    <span class="co"># Exécute la tâche jusqu&#39;à la prochaine pause</span>
    <span class="kw">def</span> run(<span class="ot">self</span>):
        <span class="kw">try</span>:
            <span class="co"># On passe la tâche à l&#39;état RUNNING et on l&#39;exécute jusqu&#39;à</span>
            <span class="co"># la prochaine suspension de la coroutine.</span>
            <span class="ot">self</span>.status = STATUS_RUNNING
            <span class="dt">next</span>(<span class="ot">self</span>.coro)
        <span class="kw">except</span> <span class="ot">StopIteration</span> <span class="ch">as</span> err:
            <span class="co"># Si la coroutine se termine, la tâche passe à l&#39;état FINISHED</span>
            <span class="co"># et on récupère sa valeur de retour.</span>
            <span class="ot">self</span>.status = STATUS_FINISHED
            <span class="ot">self</span>.return_value = err.value
        <span class="kw">except</span> <span class="ot">Exception</span> <span class="ch">as</span> err:
            <span class="co"># Si une autre exception est levée durant l&#39;exécution de la</span>
            <span class="co"># coroutine, la tâche passe à l&#39;état ERROR, et on récupère</span>
            <span class="co"># l&#39;exception pour laisser l&#39;utilisateur la traiter.</span>
            <span class="ot">self</span>.status = STATUS_ERROR
            <span class="ot">self</span>.error_value = err

    <span class="kw">def</span> is_done(<span class="ot">self</span>):
        <span class="kw">return</span> <span class="ot">self</span>.status in {STATUS_FINISHED, STATUS_ERROR}

    <span class="kw">def</span> <span class="ot">__repr__</span>(<span class="ot">self</span>):
        result = <span class="st">&#39;&#39;</span>
        <span class="kw">if</span> <span class="ot">self</span>.is_done():
            result = <span class="st">&quot; ({!r})&quot;</span>.<span class="dt">format</span>(<span class="ot">self</span>.return_value or <span class="ot">self</span>.error_value)

        <span class="kw">return</span> <span class="st">&quot;&lt;Task &#39;{}&#39; [{}]{}&gt;&quot;</span>.<span class="dt">format</span>(<span class="ot">self</span>.name, <span class="ot">self</span>.status, result)</code></pre>
<p>Son fonctionnement est plutôt simple. Réimplémentons notre boucle en nous servant de cette classe :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; task = Task(tic_tac())
&gt;&gt;&gt; task
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [NEW]&gt;
&gt;&gt;&gt; <span class="kw">while</span> not task.is_done():
...     task.run()
...     <span class="dt">print</span>(task)
...
Tic
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [RUNNING]&gt;
Tac
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [RUNNING]&gt;
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;
&gt;&gt;&gt; task.return_value
<span class="co">&#39;Boom!&#39;</span></code></pre>
<p>Bien. Nous avons une classe qui nous permet de manipuler des tâches en cours d'exécution, ces tâches étant implémentées sous la forme de coroutines. Il ne nous reste plus qu'à trouver un moyen d'exécuter plusieurs coroutines de façon <strong>concurrente</strong>, c'est-à-dire en parallèle les unes des autres. En effet, tout l'intérêt de la programmation asynchrone est d'être capable d'occuper le programme pendant qu'une tâche donnée est en attente d'un événement.</p>
<p>Pour cela, il suffit de construire une <em>file d'attente</em> de tâches à exécuter. En Python, l'objet le plus pratique pour modéliser une file d'attente est la classe standard <code>collections.deque</code> (<em>double-ended queue</em>). Cette classe possède les mêmes méthodes que les listes, auxquelles viennent s'ajouter :</p>
<ul>
<li><code>appendleft()</code> pour ajouter un élément au tout début de la liste,</li>
<li><code>popleft()</code> pour retirer (et retourner) le premier élément de la liste.</li>
</ul>
<p>Ainsi, il suffit ajouter les éléments à une extrémité de la file (<code>append()</code>), et consommer ceux de l'autre extrémité (<code>popleft()</code>). On pourrait arguer qu'il est possible d'ajouter des éléments n'importe où dans une liste avec la méthode <code>insert()</code>, mais la classe <code>deque</code> est vraiment <em>faite pour</em> créer des files et des piles : ses opérations aux extrémités sont bien plus efficaces que la méthode <code>insert()</code>.</p>
<p>Essayons d'exécuter en parallèle deux instances de notre coroutine <code>tic_tac</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="ch">from</span> collections <span class="ch">import</span> deque
&gt;&gt;&gt; running_tasks = deque()
&gt;&gt;&gt; running_tasks.append(Task(tic_tac()))
&gt;&gt;&gt; running_tasks.append(Task(tic_tac()))
&gt;&gt;&gt; <span class="kw">while</span> running_tasks:
...     <span class="co"># On récupère une tâche en attente et on l&#39;exécute</span>
...     task = running_tasks.popleft()
...     task.run()
...     <span class="kw">if</span> task.is_done():
...         <span class="co"># Si la tâche est terminée, on l&#39;affiche</span>
...         <span class="dt">print</span>(task)
...     <span class="kw">else</span>:
...         <span class="co"># La tâche n&#39;est pas finie, on la replace au bout</span>
...         <span class="co"># de la file d&#39;attente</span>
...         running_tasks.append(task)
...
Tic
Tic
Tac
Tac
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;</code></pre>
<p>Voilà qui est intéressant : la sortie des deux coroutines est entremêlée ! Cela signifie que les deux tâches ont été exécutées simultanément, de façon <strong>concurrente</strong>.</p>
<p>Nous avons tout ce qu'il nous faut pour modéliser une boucle événementielle, c'est-à-dire une boucle qui s'occupe de programmer l'exécution et le réveil des tâches dont elle a la charge. Implémentons celle-ci dans la classe <code>Loop</code> suivante :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> collections <span class="ch">import</span> deque

<span class="kw">class</span> Loop:
    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>):
        <span class="ot">self</span>._running = deque()

    <span class="kw">def</span> _loop(<span class="ot">self</span>):
        task = <span class="ot">self</span>._running.popleft()
        task.run()
        <span class="kw">if</span> task.is_done():
            <span class="dt">print</span>(task)
            <span class="kw">return</span>
        <span class="ot">self</span>.schedule(task)

    <span class="kw">def</span> run_until_empty(<span class="ot">self</span>):
        <span class="kw">while</span> <span class="ot">self</span>._running:
            <span class="ot">self</span>._loop()

    <span class="kw">def</span> schedule(<span class="ot">self</span>, task):
        <span class="kw">if</span> not <span class="dt">isinstance</span>(task, Task):
            task = Task(task)
        <span class="ot">self</span>._running.append(task)
        <span class="kw">return</span> task</code></pre>
<p>Vérifions :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> spam():
...     <span class="dt">print</span>(<span class="st">&quot;Spam&quot;</span>)
...     <span class="kw">yield</span>
...     <span class="dt">print</span>(<span class="st">&quot;Eggs&quot;</span>)
...     <span class="kw">yield</span>
...     <span class="dt">print</span>(<span class="st">&quot;Bacon&quot;</span>)
...     <span class="kw">yield</span>
...     <span class="kw">return</span> <span class="st">&quot;SPAM!&quot;</span>
...
&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.schedule(tic_tac())
&gt;&gt;&gt; event_loop.schedule(spam())
&gt;&gt;&gt; event_loop.run_until_empty()
Tic
Spam
Tac
Eggs
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;
Bacon
&lt;Task <span class="st">&#39;spam&#39;</span> [FINISHED] (<span class="st">&#39;SPAM!&#39;</span>)&gt;</code></pre>
<p>Tout fonctionne parfaitement. Dotons tout de même notre classe <code>Loop</code> d'une dernière méthode pour exécuter la boucle jusqu'à épuisement d'une coroutine en particulier :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Loop:
    <span class="co"># ...</span>
    <span class="kw">def</span> run_until_complete(<span class="ot">self</span>, task):
        task = <span class="ot">self</span>.schedule(task)
        <span class="kw">while</span> not task.is_done():
            <span class="ot">self</span>._loop()</code></pre>
<p>Testons-la :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.run_until_complete(tic_tac())
Tic
Tac
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;</code></pre>
<p>Pas de surprise.</p>
<p>Toute la programmation asynchrone repose sur ce genre de boucle qui sert en fait d'<em>ordonnanceur</em> aux tâches en cours d'exécution. Pour vous en convaincre, regardez ce bout de code qui utilise <code>asyncio</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="ch">import</span> asyncio
&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(tic_tac())
Tic
Tac
<span class="co">&#39;Boom!&#39;</span>
&gt;&gt;&gt; loop.run_until_complete(asyncio.wait([tic_tac(), spam()]))
Spam
Tic
Eggs
Tac
Bacon
({Task(&lt;tic_tac&gt;)&lt;result=<span class="st">&#39;Boom!&#39;</span>&gt;, Task(&lt;spam&gt;)&lt;result=<span class="st">&#39;SPAM!&#39;</span>&gt;}, <span class="dt">set</span>())</code></pre>
<p>Drôlement familier, n'est-ce pas ? Ne bloquez pas sur la fonction <code>asyncio.wait</code> : il s'agit simplement d'une coroutine qui sert à lancer plusieurs tâches en parallèle et attendre que celles-ci se terminent avant de retourner. Nous la reprogrammerons nous-mêmes très bientôt. ;)</p>
<h1 id="exemple-n2-exécuter-des-tâches-concurrentes-depuis-une-coroutine"><span class="header-section-number">3</span> Exemple n°2 : Exécuter des tâches concurrentes depuis une coroutine</h1>
<p>Dans le précédent exemple, nous avons déclenché l'exécution concurrente de deux coroutines en les programmant explicitement dans la boucle :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.schedule(Task(tic_tac()))
&gt;&gt;&gt; event_loop.schedule(Task(spam()))
&gt;&gt;&gt; event_loop.run_until_empty()
<span class="co"># les deux coroutines s&#39;exécutent en parallèle.</span></code></pre>
<p>Ce petit morceau de code nous a permis de prouver qu'avec une boucle événementielle et des coroutines, on avait tout ce qu'il nous fallait pour définir un modèle d'exécution concurrent.</p>
<p>Cependant, dans la pratique, on ne devrait pas avoir à manipuler directement la boucle pour réaliser quelque chose d'aussi simple que le lancement d'une tâche parallèle. Si l'on se réfère aux autres modes de concurrence :</p>
<ul>
<li>On peut lancer un <em>thread</em> et attendre la fin de l'exécution de celui-ci depuis n'importe quel <em>thread</em> en cours d'exécution.</li>
<li>On peut créer, attendre ou arrêter un processus fils depuis n'importe quel processus en cours d'exécution.</li>
</ul>
<p>Qu'il s'agisse de <em>threads</em> ou de processus, ces actions ne requièrent à aucun moment d'interagir directement avec l'ordonnanceur de votre système d'exploitation (OS).<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>Par extension, <strong>on devrait pouvoir lancer, attendre la fin de l'exécution, ou annuler une « sous-coroutine » depuis n'importe quelle coroutine en cours d'exécution</strong>, sans avoir à appeler la méthode <code>schedule()</code> ni même toucher directement à la boucle événementielle.</p>
<p>Commençons par chercher le moyen de faire appel à une coroutine depuis une tâche en cours d'exécution. Rappelons d'abord que la syntaxe <code>yield from</code> introduite dans le langage depuis Python 3.3 nous permet déjà de passer la main à une autre coroutine. Par exemple, dans le code suivant, la coroutine <code>example</code> utilise cette syntaxe pour laisser temporairement la main à la coroutine <code>subtask</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> example():
    <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;example&#39;&quot;</span>)
    <span class="dt">print</span>(<span class="st">&quot;Lancement de la tâche &#39;subtask&#39;&quot;</span>)
    <span class="kw">yield</span> <span class="ch">from</span> subtask()
    <span class="dt">print</span>(<span class="st">&quot;Retour dans &#39;example&#39;&quot;</span>)
    <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">3</span>):
        <span class="dt">print</span>(<span class="st">&quot;(example)&quot;</span>)
        <span class="kw">yield</span>

<span class="kw">def</span> subtask():
    <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;subtask&#39;&quot;</span>)
    <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">2</span>):
        <span class="dt">print</span>(<span class="st">&quot;(subtask)&quot;</span>)
        <span class="kw">yield</span></code></pre>
<p>Vérifions :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.run_until_complete(example())
Tâche <span class="st">&#39;example&#39;</span>
Lancement de la tâche <span class="st">&#39;subtask&#39;</span>
Tâche <span class="st">&#39;subtask&#39;</span>
(subtask)
(subtask)
Retour dans <span class="st">&#39;example&#39;</span>
(example)
(example)
(example)
&lt;Task <span class="st">&#39;example&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>Ainsi, Python nous fournit déjà nativement un élément de syntaxe pour <em>lancer une tâche de façon séquentielle</em> à l'intérieur d'une coroutine. Mais ce n'est pas tout à fait ce que nous voulons : notre but, ici, est de réussir à lancer la coroutine <code>subtask</code> de façon qu'elle s'exécute <em>en parallèle</em> de la coroutine <code>example</code>, et non d'attendre que <code>subtask</code> soit terminée pour reprendre l'exécution de <code>example</code>.</p>
<p>Pour ce faire, nous allons tirer parti d'une particularité des coroutines que nous avons découverte <a href="https://zestedesavoir.com/articles/232/la-puissance-cachee-des-coroutines/">dans cet article</a> : il est possible d'échanger des messages avec une coroutine en cours d'exécution.</p>
<p>Pour bien comprendre ce qui va suivre, continuons, si vous le voulez bien, notre parallèle avec le fonctionnement d'un système d'exploitation. Lorsqu'un processus<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> <code>A</code> a besoin d'exécuter un programme <code>B</code> dans un processus concurrent, celui-ci réalise ce que l'on appelle un <em>appel système</em>, c'est-à-dire qu'il <em>envoie un message</em> à l'OS pour lui demander de bien vouloir lancer le nouveau programme <code>B</code>.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> Dans la pratique, cet <em>appel système</em> ressemble à s'y méprendre à n'importe quel appel de fonction. L'idée-clé, c'est que le processus peut <em>communiquer</em> avec le noyau du système d'exploitation en échangeant des <em>messages</em> avec lui.</p>
<p>Nous allons maintenant transposer cette notion à notre cas : nous voulons réussir à faire communiquer la boucle événementielle avec les coroutines qu'elle exécute grâce à des <em>messages</em>.</p>
<p>Rappelons rapidement que l'on peut envoyer une donnée à une coroutine au moment où celle-ci se suspend, en utilisant la méthode <code>send</code> des coroutines :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> receiver():
...     <span class="kw">while</span> <span class="ot">True</span>:
...         data = <span class="kw">yield</span>
...         <span class="dt">print</span>(<span class="st">&#39;received:&#39;</span>, <span class="dt">repr</span>(data))
...
&gt;&gt;&gt; coro = receiver()
&gt;&gt;&gt; <span class="dt">next</span>(coro)  <span class="co"># La coroutine doit être démarrée pour recevoir des données</span>
&gt;&gt;&gt; coro.send(<span class="st">&quot;spam&quot;</span>)
received: <span class="st">&#39;spam&#39;</span>
&gt;&gt;&gt; coro.send(<span class="st">&quot;eggs&quot;</span>)
received: <span class="st">&#39;eggs&#39;</span>
&gt;&gt;&gt; coro.send(<span class="st">&quot;bacon&quot;</span>)
received: <span class="st">&#39;bacon&#39;</span></code></pre>
<p>À l'opposé, on peut également envoyer des données depuis une coroutine en passant un argument au mot-clé <code>yield</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> sender():
...     <span class="kw">while</span> <span class="ot">True</span>:
...        <span class="kw">yield</span> <span class="st">&quot;spam&quot;</span>
...        <span class="kw">yield</span> <span class="st">&quot;eggs&quot;</span>
...        <span class="kw">yield</span> <span class="st">&quot;bacon&quot;</span>
...
&gt;&gt;&gt; coro = sender()
&gt;&gt;&gt; coro.send(<span class="ot">None</span>)  <span class="co"># Équivalent à next(coro)</span>
<span class="co">&#39;spam&#39;</span>
&gt;&gt;&gt; coro.send(<span class="ot">None</span>)
<span class="co">&#39;eggs&#39;</span>
&gt;&gt;&gt; coro.send(<span class="ot">None</span>)
<span class="co">&#39;bacon&#39;</span></code></pre>
<p>On peut donc parfaitement imaginer simuler un appel système :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> requester():
...     data = <span class="kw">yield</span> <span class="st">&#39;REQUÊTE&#39;</span>
...     <span class="dt">print</span>(<span class="st">&#39;data:&#39;</span>, <span class="dt">repr</span>(data))
...     <span class="kw">return</span>
...
&gt;&gt;&gt; coro = requester()
&gt;&gt;&gt; coro.send(<span class="ot">None</span>)  <span class="co"># On lance la coroutine</span>
<span class="co">&#39;REQUÊTE&#39;</span>
&gt;&gt;&gt; <span class="co"># La coroutine vient de nous envoyer un message.</span>
... <span class="co"># On lui répond.</span>
...
&gt;&gt;&gt; coro.send(<span class="st">&#39;RÉPONSE&#39;</span>)
data: <span class="st">&#39;RÉPONSE&#39;</span></code></pre>
<p>Comme vous le constatez, la syntaxe de Python rend assez intuitif le fait que notre coroutine peut envoyer des requêtes à son environnement d'exécution, et se suspendre jusqu'à ce qu'on lui envoie le résultat de cette requête.</p>
<p>Pour que ces messages soient transmis par notre classe <code>Task</code> du premier exemple, nous devons l'accomoder un petit peu :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Task:
    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>):
        <span class="co"># ...</span>
        <span class="ot">self</span>.msg = <span class="ot">None</span>

    <span class="kw">def</span> run(<span class="ot">self</span>):
        <span class="kw">try</span>:
            <span class="ot">self</span>.status = STATUS_RUNNING
            <span class="kw">return</span> <span class="ot">self</span>.coro.send(<span class="ot">self</span>.msg)
        <span class="co"># ...</span></code></pre>
<p>Ainsi, on peut envoyer un message à une tâche simplement en affectant son attribut <code>task.msg</code>, et la méthode <code>task.run()</code> retourne maintenant tous les messages que la coroutine pourrait <code>yield</code>-er.</p>
<p>Reste à déterminer la forme des messages grâce auxquels la coroutine peut faire appel à sa boucle d'exécution. De manière similaire aux appels système, on peut par exemple décider que ces messages seront un tuple sous la forme suivante :</p>
<pre class="sourceCode python"><code class="sourceCode python">(TYPE_APPEL, arguments)</code></pre>
<p>Par exemple, pour demander à la boucle événementielle de lancer une ou plusieurs coroutines en parallèle, le message serait :</p>
<pre class="sourceCode python"><code class="sourceCode python">(<span class="st">&#39;SCHEDULE&#39;</span>, [task1, task2, ...])</code></pre>
<p>Modifions la méthode <code>_loop()</code> de notre boucle pour qu'elle réagisse à ces messages :</p>
<pre class="sourceCode python"><code class="sourceCode python">CALL_SCHEDULE = <span class="st">&#39;SCHEDULE&#39;</span>

<span class="kw">class</span> Loop:
    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>):
        <span class="ot">self</span>._running = deque()

    <span class="kw">def</span> _loop(<span class="ot">self</span>):
        task = <span class="ot">self</span>._running.popleft()
        msg = task.run()  <span class="co"># Réception du message</span>

        <span class="kw">if</span> task.is_done():
            <span class="dt">print</span>(task)
        <span class="kw">else</span>:
            <span class="ot">self</span>.schedule(task)

        <span class="kw">if</span> not msg:
            <span class="kw">return</span>

        msg_type, args = msg
        <span class="kw">if</span> msg_type == CALL_SCHEDULE:
            <span class="co"># Si le message est un ordre d&#39;exécuter des tâches parallèles,</span>
            <span class="co"># on programme ces tâches et on les retourne à la tâche</span>
            <span class="co"># appelante.</span>
            task.msg = <span class="dt">tuple</span>(<span class="ot">self</span>.schedule(subtask) <span class="kw">for</span> subtask in args)
        <span class="kw">else</span>:
            <span class="kw">raise</span> <span class="ot">RuntimeError</span>(<span class="st">&quot;Message inconnu : {}&quot;</span>.<span class="dt">format</span>(msg_type))</code></pre>
<p>On peut maintenant abstraire ce message derrière une coroutine que nous appellerons <code>ensure_future()</code>, pour respecter la même nomenclature qu'<code>asyncio</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> ensure_future(task):
    <span class="co"># L&#39;appel à CALL_SCHEDULE retourne un tuple à un seul élément dans ce cas</span>
    (task,) = <span class="kw">yield</span> CALL_SCHEDULE, [task]
    <span class="kw">return</span> task</code></pre>
<p>Nous disposons désormais de deux façons d'exécuter une sous-tâche depuis une coroutine :</p>
<ul>
<li><p><code>yield from subtask()</code> : lance l'exécution d'une coroutine de façon <em>séquentielle</em>, c'est-à-dire en lui laissant la main jusqu'à ce que celle-ci se soit terminée.</p></li>
<li><p><code>yield from ensure_future(subtask())</code> : lance l'exécution d'une coroutine de <em>en parallèle</em>.</p></li>
</ul>
<p>Ainsi, si nous modifions notre coroutine <code>example()</code> définie plus haut en conséquence, nous pouvons vérifier que nos tâches sont bien exécutées de façon concurrente :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> example():
...     <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;example&#39;&quot;</span>)
...     <span class="dt">print</span>(<span class="st">&quot;Lancement de la tâche &#39;subtask&#39;&quot;</span>)
...     <span class="kw">yield</span> <span class="ch">from</span> ensure_future(subtask())
...     <span class="dt">print</span>(<span class="st">&quot;Retour dans &#39;example&#39;&quot;</span>)
...     <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">3</span>):
...         <span class="dt">print</span>(<span class="st">&quot;(example)&quot;</span>)
...         <span class="kw">yield</span>
...
&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.run_until_complete(example())
Tâche <span class="st">&#39;example&#39;</span>
Lancement de la tâche <span class="st">&#39;subtask&#39;</span>
Retour dans <span class="st">&#39;example&#39;</span>
(example)
Tâche <span class="st">&#39;subtask&#39;</span>
(subtask)
(example)
(subtask)
(example)
&lt;Task <span class="st">&#39;subtask&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
&lt;Task <span class="st">&#39;example&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>Magique, n'est-ce pas ?</p>
<p>Par contre, une fois que notre coroutine est lancée, nous n'avons pas tout à fait le contrôle de son exécution. Par exemple, si nous rendions la tâche <code>subtask</code> plus longue qu'<code>example</code>, celle-ci lui « survivrait » :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> subtask():
...     <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;subtask&#39;&quot;</span>)
...     <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">5</span>):
...         <span class="dt">print</span>(<span class="st">&quot;(subtask)&quot;</span>)
...         <span class="kw">yield</span>
...
&gt;&gt;&gt; event_loop.run_until_complete(example())
Tâche <span class="st">&#39;example&#39;</span>
Lancement de la tâche <span class="st">&#39;subtask&#39;</span>
Retour dans <span class="st">&#39;example&#39;</span>
(example)
Tâche <span class="st">&#39;subtask&#39;</span>
(subtask)
(example)
(subtask)
(example)
(subtask)
&lt;Task <span class="st">&#39;example&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
&gt;&gt;&gt; <span class="co"># L&#39;exécution s&#39;arrête avec la fin de la coroutine &#39;example&#39;</span>
... <span class="co"># Vidons ce qu&#39;il reste dans la boucle événementielle :</span>
...
&gt;&gt;&gt; event_loop.run_until_empty()
(subtask)
(subtask)
&lt;Task <span class="st">&#39;subtask&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p><strong>Que faire si nous ne voulons pas qu'une coroutine quitte avant une sous-tâche qu'elle aurait lancée en parallèle ?</strong></p>
<p>Nous avons deux solutions. La première, dont nous nous contenterons dans cet exemple, serait de pouvoir <em>annuler</em> une tâche en cours d'exécution. Il nous suffit pour cela de créer un nouvel état dans notre classe <code>Task</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">STATUS_CANCELLED = <span class="st">&quot;CANCELLED&quot;</span>

<span class="kw">class</span> Task:

    <span class="co"># ...</span>

    <span class="kw">def</span> cancel(<span class="ot">self</span>):
        <span class="kw">if</span> <span class="ot">self</span>.is_done():
            <span class="co"># Inutile d&#39;annuler une tâche déjà terminée</span>
            <span class="kw">return</span>
        <span class="ot">self</span>.status = STATUS_CANCELLED

    <span class="kw">def</span> is_cancelled(<span class="ot">self</span>):
        <span class="kw">return</span> <span class="ot">self</span>.status == STATUS_CANCELLED</code></pre>
<p>Rajoutons un test dans la boucle événementielle pour déprogrammer les tâches annulées :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Loop:

    <span class="co"># ...</span>

    <span class="kw">def</span> _loop(<span class="ot">self</span>):
        task = <span class="ot">self</span>._running.popleft()

        <span class="kw">if</span> task.is_cancelled():
            <span class="co"># Si la tâche a été annulée,</span>
            <span class="co"># on ne l&#39;exécute pas et on &quot;l&#39;oublie&quot;.</span>
            <span class="dt">print</span>(task)
            <span class="kw">return</span>

        <span class="co"># ... le reste de la méthode est identique</span></code></pre>
<p>Il ne nous reste plus qu'une petite coroutine utilitaire à écrire pour annuler une tâche en cours d'exécution :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> cancel(task):
    <span class="co"># On annule la tâche</span>
    task.cancel()
    <span class="co"># On laisse la main à la boucle événementielle pour lui laisser l&#39;occasion</span>
    <span class="co"># de prendre en compte l&#39;annulation</span>
    <span class="kw">yield</span>

<span class="kw">def</span> example():
    <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;example&#39;&quot;</span>)
    <span class="dt">print</span>(<span class="st">&quot;Lancement de la tâche &#39;subtask&#39;&quot;</span>)
    sub = <span class="kw">yield</span> <span class="ch">from</span> ensure_future(subtask())
    <span class="dt">print</span>(<span class="st">&quot;Retour dans &#39;example&#39;&quot;</span>)
    <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">3</span>):
        <span class="dt">print</span>(<span class="st">&quot;(example)&quot;</span>)
        <span class="kw">yield</span>
    <span class="kw">yield</span> <span class="ch">from</span> cancel(sub)</code></pre>
<p>Vérifions :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop.run_until_complete(example())
Tâche <span class="st">&#39;example&#39;</span>
Lancement de la tâche <span class="st">&#39;subtask&#39;</span>
Retour dans <span class="st">&#39;example&#39;</span>
(example)
Tâche <span class="st">&#39;subtask&#39;</span>
(subtask)
(example)
(subtask)
(example)
(subtask)
&lt;Task <span class="st">&#39;subtask&#39;</span> [CANCELLED]&gt;
&lt;Task <span class="st">&#39;example&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>Notre mécanisme d'annulation fonctionne comme prévu. Cela dit, on peut aussi imaginer tout simplement vouloir <em>attendre</em> de façon asynchrone que la sous-tâche ait terminé son exécution avant de quitter proprement…</p>
<p>Mais ça, je vous le garde pour le prochain exemple. ;)</p>
<h1 id="exemple-n3-attendre-de-façon-asynchrone"><span class="header-section-number">4</span> Exemple n°3 : Attendre de façon asynchrone</h1>
<p>Dans l'exemple n°1, nous avons vu un bref appel à une fonction d'<code>asyncio</code> :</p>
<pre class="python3"><code>&gt;&gt;&gt; import asyncio
&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(asyncio.wait([tic_tac(), spam()]))
Tic
spam
Tac
eggs
bacon
({Task(&lt;tic_tac&gt;)&lt;result=&#39;Boum!&#39;&gt;, Task(&lt;spam&gt;)&lt;result=&#39;spam&#39;&gt;}, set())</code></pre>
<p>La fonction intéressante de cet exemple est <code>asyncio.wait()</code>. Celle-ci permet d'attendre qu'une ou plusieurs tâches (lancées en parallèle) soient terminées. Sa valeur de retour se compose de deux ensembles (<code>set()</code>) :</p>
<ul>
<li>Le premier contient les tâches qui se sont terminées normalement ;</li>
<li>Le second contient les tâches qui ne sont pas encore terminées, ont été annulées ou ont quitté sur une erreur.</li>
</ul>
<p>C'est cette fonction que nous allons implémenter maintenant.</p>
<p>En fait, le plus difficile dans cette fonction est surtout sa partie cosmétique. Pour avoir un comportement souple, il faut gérer le cas où les tâches passées à cette fonction :</p>
<ul>
<li>Sont des coroutines et non des instances de la classe <code>Task</code>,</li>
<li>N'ont pas encore été programmées pour être exécutées par la boucle,</li>
<li>Sont déjà en train de s'exécuter.</li>
</ul>
<p>Voilà ce que cela peut donner :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> wait(tasks):
    <span class="co"># On lance les tâches qui ne l&#39;ont pas encore été</span>
    <span class="kw">for</span> idx, task in <span class="dt">enumerate</span>(tasks):
        <span class="co"># Si c&#39;est une coroutine, on crée la tâche correspondante</span>
        <span class="kw">if</span> not <span class="dt">isinstance</span>(task, Task):
            task = tasks[idx] = Task(task)
        <span class="co"># Si la tâche n&#39;a pas encore été lancée, on la lance</span>
        <span class="kw">if</span> task.status == STATUS_NEW:
            <span class="kw">yield</span> <span class="ch">from</span> launch(task)

    <span class="co"># On attend que toutes les tâches soient terminées</span>
    <span class="kw">while</span> not <span class="dt">all</span>(task.is_done() <span class="kw">for</span> task in tasks):
        <span class="kw">yield</span>

    <span class="co"># On crée les deux ensembles pour le résultat</span>
    finished = <span class="dt">set</span>()
    error = <span class="dt">set</span>()
    <span class="kw">for</span> task in tasks:
        <span class="kw">if</span> task.status == STATUS_FINISHED:
            finished.add(task)
        <span class="kw">elif</span> task.status == STATUS_ERROR:
            error.add(task)

    <span class="kw">return</span> finished, error</code></pre>
<p>Nous pouvons vérifier que cette nouvelle fonction marche comme prévu avec les coroutines suivantes.</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> repeat(msg, times):
...    <span class="dt">print</span>(<span class="st">&quot;lancement de la tâche&quot;</span>, <span class="dt">repr</span>(msg))
...     <span class="kw">for</span> _ in <span class="dt">range</span>(times):
...         <span class="dt">print</span>(msg)
...         <span class="kw">yield</span>
...
&gt;&gt;&gt; <span class="kw">def</span> main_task():
...     <span class="dt">print</span>(<span class="st">&quot;Lancement de 3 tâches en parallèle&quot;</span>)
...     returned, error = <span class="kw">yield</span> <span class="ch">from</span> wait([
...         repeat(<span class="st">&quot;spam&quot;</span>, <span class="dv">10</span>),
...         repeat(<span class="st">&quot;eggs&quot;</span>, <span class="dv">3</span>),
...         repeat(<span class="st">&quot;bacon&quot;</span>, <span class="dv">5</span>),
...     ])
...     <span class="dt">print</span>(<span class="st">&quot;Retour à la fonction principale: {} OK, {} erreurs&quot;</span>.<span class="dt">format</span>(
...         <span class="dt">len</span>(returned), <span class="dt">len</span>(error)
...     ))
...</code></pre>
<p>Et le résultat :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop = Loop()
&gt;&gt;&gt; loop.schedule(main_task())
&gt;&gt;&gt; loop.run_once()
Lancement de <span class="dv">3</span> tâches en parallèle
lancement de la tâche <span class="st">&#39;spam&#39;</span>
spam
lancement de la tâche <span class="st">&#39;eggs&#39;</span>
eggs
spam
lancement de la tâche <span class="st">&#39;bacon&#39;</span>
bacon
eggs
spam
bacon
eggs
spam
bacon
&lt;Task <span class="st">&#39;repeat&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
spam
bacon
spam
bacon
spam
&lt;Task <span class="st">&#39;repeat&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
spam
spam
spam
&lt;Task <span class="st">&#39;repeat&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
Retour à la fonction principale: <span class="dv">3</span> OK, <span class="dv">0</span> erreurs
&lt;Task <span class="st">&#39;main_task&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
&gt;&gt;&gt;</code></pre>
<p>En guise d'exercice, si cela vous intéresse, vous pouvez :</p>
<ul>
<li>Essayer d'expliquer pourquoi les tâches ne démarrent pas toutes en même temps,</li>
<li>Constater que si vous réimplémentez cet exemple avec <code>asyncio</code>, ce problème ne se produit pas,</li>
<li>Modifier la fonction <code>wait()</code> et la boucle événementielle pour coller au comportement d'<code>asyncio</code>.</li>
</ul>
<p>En ce qui nous concerne, la fonction <code>wait</code> que nous venons d'implémenter nous suffira largement pour la suite.</p>
<h1 id="exemple-n4-modélisons-le-serveur-du-fast-food-avec-asyncio"><span class="header-section-number">5</span> Exemple n°4 : Modélisons le serveur du <em>fast food</em> avec <code>asyncio</code></h1>
<p>Maintenant que nous avons compris comment fonctionne sa boucle événementielle, il est temps de nous mettre en jambes avec <code>asyncio</code> en l'utilisant pour modéliser l'exemple du début de cet article : l'employé de <em>fast food</em>.</p>
<p>Voici d'abord à quoi ressemble le programme dans sa version synchrone.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> time <span class="ch">import</span> sleep
<span class="ch">from</span> datetime <span class="ch">import</span> datetime

<span class="kw">def</span> get_burger(client):
    <span class="dt">print</span>(<span class="st">&quot;&gt; Commande du burger pour &#39;{}&#39; en cuisine&quot;</span>.<span class="dt">format</span>(client))
    sleep(<span class="dv">4</span>)
    <span class="dt">print</span>(<span class="st">&quot;&lt; Le burger de &#39;{}&#39; est prêt&quot;</span>.<span class="dt">format</span>(client))

<span class="kw">def</span> get_fries(client):
    <span class="dt">print</span>(<span class="st">&quot;&gt; Mettre des frites à cuire pour {}&quot;</span>.<span class="dt">format</span>(client))
    sleep(<span class="dv">8</span>)
    <span class="dt">print</span>(<span class="st">&quot;&lt; Les frites de &#39;{}&#39; sont prêtes&quot;</span>.<span class="dt">format</span>(client))

<span class="kw">def</span> get_soda(client):
    <span class="dt">print</span>(<span class="st">&quot;&gt; Remplissage du gobelet de soda pour {}&quot;</span>.<span class="dt">format</span>(client))
    sleep(<span class="dv">2</span>)
    <span class="dt">print</span>(<span class="st">&quot;&lt; Le soda de &#39;{}&#39; est prêt&quot;</span>.<span class="dt">format</span>(client))

<span class="kw">def</span> serve(client):
    <span class="dt">print</span>(<span class="st">&quot;Préparation de la commande de &#39;{}&#39;&quot;</span>.<span class="dt">format</span>(client))
    start = datetime.now()
    get_burger(client)
    get_fries(client)
    get_soda(client)
    duration = datetime.now() - start
    <span class="dt">print</span>(<span class="st">&quot;Commande de &#39;{}&#39; prête en {}&quot;</span>.<span class="dt">format</span>(client, duration))</code></pre>
<p>Résultat : la commande est prête en 14 secondes.</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; serve(<span class="st">&#39;A&#39;</span>)
Préparation de la commande de <span class="st">&#39;A&#39;</span>
&gt; Commande du burger pour <span class="st">&#39;A&#39;</span> en cuisine
&lt; Le burger de <span class="st">&#39;A&#39;</span> est prêt
&gt; Mettre des frites à cuire pour A
&lt; Les frites de <span class="st">&#39;A&#39;</span> sont prêtes
&gt; Remplissage du gobelet de soda pour A
&lt; Le soda de <span class="st">&#39;A&#39;</span> est prêt
Commande de <span class="st">&#39;A&#39;</span> prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">14.015165</span></code></pre>
<p>Il ne nous reste plus qu'à implémenter cet exemple avec <code>asyncio</code>. Le premier jet n'est pas très difficile : il faut juste penser à utiliser la coroutine <code>asyncio.sleep()</code> plutôt que <code>time.sleep()</code> pour que l'endormissement de la tâche ne soit pas bloquant.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">import</span> asyncio
<span class="ch">from</span> datetime <span class="ch">import</span> datetime

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_burger(client):
    <span class="dt">print</span>(<span class="st">&quot;&gt; Commande du burger pour {} en cuisine&quot;</span>.<span class="dt">format</span>(client))
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">4</span>)
    <span class="dt">print</span>(<span class="st">&quot;&lt; Le burger de {} est prêt&quot;</span>.<span class="dt">format</span>(client))

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_fries(client):
    <span class="dt">print</span>(<span class="st">&quot;&gt; Mettre des frites à cuire pour {}&quot;</span>.<span class="dt">format</span>(client))
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">8</span>)
    <span class="dt">print</span>(<span class="st">&quot;&lt; Les frites de {} sont prêtes&quot;</span>.<span class="dt">format</span>(client))

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_soda(client):
    <span class="dt">print</span>(<span class="st">&quot;&gt; Remplissage du gobelet de soda pour {}&quot;</span>.<span class="dt">format</span>(client))
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">2</span>)
    <span class="dt">print</span>(<span class="st">&quot;&lt; Le soda de {} est prêt&quot;</span>.<span class="dt">format</span>(client))

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> serve(client):
    <span class="dt">print</span>(<span class="st">&quot;Préparation de la commande de {}&quot;</span>.<span class="dt">format</span>(client))
    start = datetime.now()
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.wait([
        get_burger(client),
        get_fries(client),
        get_soda(client),
    ])
    duration = datetime.now() - start
    <span class="dt">print</span>(<span class="st">&quot;Commande de {} prête en {}&quot;</span>.<span class="dt">format</span>(client, duration))</code></pre>
<p>Remarquez que nous avons décoré toutes nos coroutines avec le décorateur <code>@asyncio.coroutine</code> : il s'agit d'une convention pour distinguer les fonctions synchrones des coroutines asynchrones dans du code utilisant <code>asyncio</code>. En dehors de l'indice visuel, ce décorateur permet de s'assurer qu'une fonction donnée sera considérée par <code>asyncio</code> comme une coroutine, même si celle-ci ne <code>yield</code> jamais.</p>
<p>Exécutons-le maintenant :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="ch">import</span> asyncio
&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(serve(<span class="st">&#39;A&#39;</span>))
Préparation de la commande de A
&gt; Commande du burger pour A en cuisine
&gt; Mettre des frites à cuire pour A
&gt; Remplissage du gobelet de soda pour A
&lt; Le soda de A est prêt
&lt; Le burger de A est prêt
&lt; Les frites de A sont prêtes
Commande de A prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.005791</span></code></pre>
<p>Nous sommes passés de 14 secondes à 8 secondes.</p>
<p>Et pour servir deux clients à la fois ?</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop.run_until_complete(asyncio.wait([serve(<span class="st">&#39;A&#39;</span>), serve(<span class="st">&#39;B&#39;</span>)]))
Préparation de la commande de B
Préparation de la commande de A
&gt; Mettre des frites à cuire pour B
&gt; Commande du burger pour B en cuisine
&gt; Remplissage du gobelet de soda pour B
&gt; Remplissage du gobelet de soda pour A
&gt; Commande du burger pour A en cuisine
&gt; Mettre des frites à cuire pour A
&lt; Le soda de B est prêt
&lt; Le soda de A est prêt
&lt; Le burger de B est prêt
&lt; Le burger de A est prêt
&lt; Les frites de B sont prêtes
&lt; Les frites de A sont prêtes
Commande de B prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.005967</span>
Commande de A prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.005940</span></code></pre>
<p>Les deux ont mis tout autant de temps, néanmoins cet affichage ne semble pas très réaliste.</p>
<p>En effet :</p>
<ul>
<li>la machine à soda ne peut faire couler qu'un seul soda à la fois, or ici les deux sodas ont été préparés simultanément.</li>
<li>la cuisine ne comporte que 3 cuisiniers, donc on ne peut avoir au maximum que 3 hamburgers en cours de préparation à un instant donné.</li>
<li>le bac à frites fait cuire en général 5 portions de frites en même temps, or ici il a été utilisé deux fois en parallèle pour produire uniquement deux portions de frites.</li>
</ul>
<p>Comment modéliser ces contraintes ?</p>
<p>La machine à soda est certainement la plus simple. Il est possible de verrouiller une ressource de manière à ce qu'une seule tâche puisse y accéder à la fois, en utilisant ce que l'on appelle un <strong>verrou</strong> (<code>asyncio.Lock</code>). Plaçons un verrou sur notre machine à soda :</p>
<pre class="sourceCode python"><code class="sourceCode python">SODA_LOCK = asyncio.Lock()

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_soda(client):
    <span class="co"># Acquisition du verrou</span>
    <span class="kw">with</span> (<span class="kw">yield</span> <span class="ch">from</span> SODA_LOCK):
        <span class="co"># Une seule tâche à la fois peut exécuter ce bloc</span>
        <span class="dt">print</span>(<span class="st">&quot;&gt; Remplissage du gobelet de soda pour {}&quot;</span>.<span class="dt">format</span>(client))
        <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">2</span>)
        <span class="dt">print</span>(<span class="st">&quot;&lt; Le soda de {} est prêt&quot;</span>.<span class="dt">format</span>(client))</code></pre>
<p>Le <code>with (yield from SODA_LOCK)</code> signifie que lorsque le serveur arrive à la machine à soda pour y déposer un gobelet :</p>
<ul>
<li>soit la machine est libre (déverrouillée), auquel cas il peut la verrouiller pour l'utiliser immédiatement,</li>
<li>soit celle-ci est déjà en train de fonctionner, auquel cas il attend que le soda en cours de préparation soit prêt avant de se servir de la machine.</li>
</ul>
<p>Passons à la cuisine. Seuls 3 burgers peuvent être fabriqués en même temps. Cela peut se modéliser en utilisant un <strong>sémaphore</strong> (<code>asyncio.Semaphore</code>), qui est une sorte de &quot;verrou multiple&quot;. On l'utilise pour qu'au plus N tâches puissent exécuter un morceau de code à un instant donné.</p>
<pre class="sourceCode python"><code class="sourceCode python">BURGER_SEM = asyncio.Semaphore(<span class="dv">3</span>)

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_burger(client):
    <span class="dt">print</span>(<span class="st">&quot;&gt; Commande du burger pour {} en cuisine&quot;</span>.<span class="dt">format</span>(client))
    <span class="kw">with</span> (<span class="kw">yield</span> <span class="ch">from</span> BURGER_SEM):
        <span class="dt">print</span>(<span class="st">&quot;* Le burger de {} est en préparation&quot;</span>.<span class="dt">format</span>(client))
        <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">4</span>)
        <span class="dt">print</span>(<span class="st">&quot;&lt; Le burger de {} est prêt&quot;</span>.<span class="dt">format</span>(client))</code></pre>
<p>Le <code>with (yield from BURGER_SEM)</code> veut dire que lorsqu'une commande est passée en cuisine :</p>
<ul>
<li>soit il y a un cuisinier libre, et celui-ci commence immédiatement à préparer le hamburger,</li>
<li>soit tous les cuisiniers sont occupés, auquel cas on attend qu'il y en ait un qui se libère pour s'occuper de notre hamburger.</li>
</ul>
<p>Passons enfin au bac à frites. Cette fois, <code>asyncio</code> ne nous fournira pas d'objet magique, donc il va nous falloir réfléchir un peu plus. Il faut que l'on puisse l'utiliser <em>une fois</em> pour faire les frites des 5 prochaines commandes. Dans ce cas, un compteur semble une bonne idée :</p>
<ul>
<li>Chaque fois que l'on prend une portion de frites, on décrémente le compteur ;</li>
<li>S'il n'y a plus de frites dans le bac, il faut en refaire.</li>
</ul>
<p>Mais attention, si les frites sont déjà en cours de préparation, il est inutile de lancer une nouvelle fournée !</p>
<p>Voici comment on pourrait s'y prendre :</p>
<pre class="sourceCode python"><code class="sourceCode python">FRIES_PARTS = <span class="dv">0</span>
FRIES_LOCK = asyncio.Lock()

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_fries(client):
    <span class="kw">global</span> FRIES_PARTS
    <span class="kw">with</span> (<span class="kw">yield</span> <span class="ch">from</span> FRIES_LOCK):
        <span class="dt">print</span>(
            <span class="st">&quot;&gt; Récupération d&#39;une portion de frites pour {}&quot;</span>.<span class="dt">format</span>(client)
        )
        <span class="kw">if</span> FRIES_PARTS == <span class="dv">0</span>:
            <span class="dt">print</span>(<span class="st">&quot;* Mettre des frites à cuire&quot;</span>)
            <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">8</span>)
            FRIES_PARTS = <span class="dv">5</span>
            <span class="dt">print</span>(<span class="st">&quot;* Les frites sont prêtes&quot;</span>))
        FRIES_PARTS -= <span class="dv">1</span>
        <span class="dt">print</span>(<span class="st">&quot;&lt; Les frites de {} sont prêtes&quot;</span>.<span class="dt">format</span>(client))</code></pre>
<p>Dans cet exemple, on place un verrou sur le bac à frites pour qu'un seul serveur puisse y accéder à la fois. Lorsqu'un serveur arrive devant le bac à frites, soit celui-ci contient encore des portions de frites, auquel cas il en récupère une et retourne immédiatement, soit le bac est vide, donc le serveur met des frites à cuire avant de pouvoir en récupérer une portion.</p>
<p>Voyons voir ce que cela donne à l'exécution :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop.run_until_complete(asyncio.wait([serve(<span class="st">&#39;A&#39;</span>), serve(<span class="st">&#39;B&#39;</span>)]))
Préparation de la commande de B
Préparation de la commande de A
&gt; Remplissage du gobelet de soda pour B
&gt; Commande du burger pour B en cuisine
* Le burger de B est en préparation
&gt; Récupération d<span class="st">&#39;une portion de frites pour B</span>
<span class="st">* Mettre des frites à cuire</span>
<span class="st">&gt; Commande du burger pour A en cuisine</span>
<span class="st">* Le burger de A est en préparation</span>
<span class="st">&lt; Le soda de B est prêt</span>
<span class="st">&gt; Remplissage du gobelet de soda pour A</span>
<span class="st">&lt; Le burger de B est prêt</span>
<span class="st">&lt; Le burger de A est prêt</span>
<span class="st">&lt; Le soda de A est prêt</span>
<span class="st">* Les frites sont prêtes</span>
<span class="st">&lt; Les frites de B sont prêtes</span>
<span class="st">&gt; Récupération d&#39;</span>une portion de frites pour A
&lt; Les frites de A sont prêtes
Commande de B prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.006742</span>
Commande de A prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.006859</span>
({Task(&lt;serve&gt;)&lt;result=<span class="ot">None</span>&gt;, Task(&lt;serve&gt;)&lt;result=<span class="ot">None</span>&gt;}, <span class="dt">set</span>())</code></pre>
<p>Et voilà. Nos deux tâches prennent le même temps, mais s'arrangent pour ne pas accéder simultanément à la machine à sodas ni au bac à frites.</p>
<p>Voyons maintenant ce que cela donne si 10 clients passent commande en même temps :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop.run_until_complete(
...     asyncio.wait([serve(clt) <span class="kw">for</span> clt in <span class="st">&#39;ABCDEFGHIJ&#39;</span>])
... )
...
<span class="co"># ...</span>
Commande de C prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.009554</span>
Commande de G prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.009934</span>
Commande de B prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.010281</span>
Commande de D prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.014250</span>
Commande de H prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">10.017237</span>
Commande de I prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">16.014170</span>
Commande de E prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">16.014511</span>
Commande de A prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">16.022411</span>
Commande de J prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">18.023141</span>
Commande de F prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">20.026096</span></code></pre>
<p>On se rend compte que les performances de notre serveur de fast-food se dégradent plus ou moins, même si on est toujours loin des 140 secondes que le serveur aurait pris s'il avait traité toutes ces commandes de façon synchrone.</p>
<p>Cela dit, il est plutôt rare que les clients passent leurs commandes tous en même temps. Une modélisation plus proche de la réalité serait que ces dix commandes arrivent à deux secondes d'intervalle :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> test():
    tasks = []
    <span class="kw">for</span> client in <span class="st">&#39;ABCDEFGHIJ&#39;</span>:
        <span class="co"># appel équivalent à notre fonction `launch()`</span>
        task = asyncio.async(serve(client))
        tasks.append(task)
        <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">2</span>)
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.wait(tasks)</code></pre>
<p>Dans ces conditions, les temps d'attente individuels de chaque client sont assez largement réduits :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop = asyncio.run_until_complete(test())
Préparation de la commande de A
Préparation de la commande de B
Préparation de la commande de C
Préparation de la commande de D
Commande de A prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.004599</span>
Commande de B prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">06.002353</span>
Préparation de la commande de E
Commande de C prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.002907</span>
Préparation de la commande de F
Commande de D prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.004343</span>
Préparation de la commande de G
Commande de E prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.004445</span>
Préparation de la commande de H
Préparation de la commande de I
Commande de F prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.005227</span>
Commande de G prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">06.005087</span>
Préparation de la commande de J
Commande de H prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.003595</span>
Commande de I prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.003620</span>
Commande de J prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.004184</span></code></pre>
<p>À raison d'un client toutes les deux secondes, notre serveur est capable de traiter des commandes avec un temps moyen de 5.2 secondes, le maximum étant 8 secondes et le minimum à 4 secondes.</p>
<p>Et si nous <em>stressions</em> un peu notre serveur et qu'on lui passait une commande par seconde ?</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> test():
    tasks = []
    <span class="kw">for</span> client in <span class="st">&#39;ABCDEFGHIJ&#39;</span>:
        task = asyncio.async(serve(client))
        tasks.append(task)
        <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">1</span>)
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.wait(tasks)</code></pre>
<p>Comme prévu, celui-ci, sous la pression, est un peu moins efficace à cause des contraintes des différentes machines qu'il utilise, même si l'on reste tout à fait loin de l'inefficacité pathologique de la version synchrone :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(test())
Préparation de la commande de A
Préparation de la commande de B
Préparation de la commande de C
Préparation de la commande de D
Préparation de la commande de E
Préparation de la commande de F
Préparation de la commande de G
Préparation de la commande de H
Commande de A prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.003498</span>
Commande de B prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">07.001989</span>
Commande de C prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">06.000267</span>
Commande de D prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">05.004673</span>
Préparation de la commande de I
Préparation de la commande de J
Commande de E prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">06.005059</span>
Commande de F prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">11.002432</span>
Commande de G prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">10.002475</span>
Commande de H prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">09.000938</span>
Commande de I prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">10.002911</span>
Commande de J prête en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">11.004703</span></code></pre>
<p>On peut se demander à quel endroit le service est ralenti quand on sert 1 client par seconde :</p>
<ul>
<li>S'agit-il de la machine à sodas, qui produit un soda toutes les 2 secondes ?</li>
<li>S'agit-il du bac à frites, qui peut produire 5 portions en 8 secondes ?</li>
<li>Ou bien s'agit-il de la cuisine, qui produit un hamburger en 4 secondes, mais reste limitée à trois cuisiniers ?</li>
</ul>
<p>Les réponses à ces questions vous sont laissées en guise d'exercice. Vous pouvez essayer d'apporter les modifications suivantes au modèle, pour aider le gérant du restaurant à optimiser son service :</p>
<ul>
<li>Mettre en place une seconde machine à sodas aussi rapide que la première.</li>
<li>Acheter un nouveau bac à frites pouvant produire 6 portions en 7 secondes.</li>
<li>Embaucher un quatrième cuisinier.</li>
</ul>
<p>Bon courage !</p>
<h1 id="exemple-n5-les-entréessorties-le-nerf-de-la-guerre"><span class="header-section-number">6</span> Exemple n°5 : Les entrées/sorties, le nerf de la guerre</h1>
<p>Vous vous demandez peut-être pourquoi on parle tout le temps d'<em>IO</em> en programmation asynchrone. En effet, les entrées/sorties des programmes semblent indissociables du concept d'asynchrone, à tel point que cela se traduit jusque dans le nom de la bibliothèque standard <code>asyncio</code> de Python. Mais <em>pourquoi</em> ?</p>
<p>Commençons par une définition : une <em>IO</em>, c'est une opération pendant laquelle un programme <em>interagit avec un flux de données</em>. Ce <strong>flux de données</strong> peut être plein de choses : une connexion réseau, les flux standard <code>STDIN</code>, <code>STDOUT</code> ou <code>STDERR</code> du processus en cours d'exécution, un fichier, ou même une abstraction matérielle<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>. « Interagir avec un flux de données », ça veut dire l'<strong>ouvrir</strong>, <strong>lire</strong> ou <strong>écrire</strong> dedans ou le <strong>fermer</strong>.</p>
<p>Jusqu'ici, nous avons travaillé sur des exemples très simples qui se contentaient d'afficher des choses à l'écran pour bien comprendre l'ordre dans lequel les instructions étaient exécutées. Nos tâches ne réalisaient du coup que des entrées/sorties, certes, mais celles-ci étaient <em>synchrones</em> : on a considéré jusqu'à maintenant qu'un <code>print()</code> dans la console s'exécute immédiatement et sans délai lors de son appel, ce qui est parfaitement intuitif...</p>
<p>... mais pas toujours le reflet de la réalité.</p>
<p>Prenons par exemple une IO très simple que vous réalisez en permanence sur votre ordinateur ou smartphone sans même vous en rendre compte : <strong>que se passe-t-il entre le moment où vous avez cliqué sur un lien dans une page web, et celui où le résultat commence à s'afficher sur votre écran</strong> ?</p>
<p>Eh bien vous <strong>attendez</strong>. Tout simplement. Et votre navigateur aussi. Sans rentrer dans le détail du protocole HTTP, on peut schématiser grossièrement ce qui se passe comme ceci :</p>
<pre><code>        Navigateur                           Serveur web
        ==========                           ===========

           [clic sur le lien]                     .
[création d&#39;une requête HTTP]                     .
       [connexion au serveur]  ------&gt;            .
            .                            [connexion reçue]
            .                  &lt;------   [connexion acceptée]
        [envoi de la requête]  ------&gt;            .
            .                            [réception de la requête]
            .                            [création de la réponse]
            .                  &lt;------   [envoi de la réponse]
    [réception de la réponse]                     .
       [affichage de la page]</code></pre>
<p>Dans ce schéma, tous les points (<code>.</code>) symbolisent une attente. Un échange HTTP (et plus généralement une IO), c'est une opération pendant laquelle les programmes, passent le plus clair de leur temps à <strong>ne rien faire</strong>. Et votre navigateur lui-même vous le dit (généralement dans un petit cadre en bas à gauche de l'écran) :</p>
<figure>
<img src="src/img/waiting.png" />
</figure>
<p>Dans ces conditions, l'idée de base de la programmation asynchrone est de <em>mettre à profit</em> tout ce temps que l'on passe à attendre pendant la réalisation d'une IO pour <strong>s'occuper en faisant autre chose</strong>.</p>
<p>L'exemple du serveur de <em>fast food</em> que nous avons modélisé plus tôt n'est pas anodin ; qu'il s'agisse du serveur <em>bien réel</em> d'un restaurant ou celui d'une application réseau, les deux réalisent en général des opérations comparables. En effet, de très nombreux serveurs (par exemple d'applications Web) fonctionnent plus ou moins suivant ce schéma :</p>
<ol type="1">
<li>Recevoir une requête, une commande ou un message,</li>
<li>Aller récupérer des ressources à différents endroits,</li>
<li>Combiner les ressources entre elles,</li>
<li>Répondre au client.</li>
</ol>
<p>Dans ce schéma, les points 1, 2 et 4 peuvent être des <em>IO</em> :</p>
<ol type="1">
<li>Réception :
<ul>
<li><strong>Attente</strong> d'une connexion,</li>
<li>Acceptation de la connexion,</li>
<li><strong>Attente</strong> du message,</li>
<li>Réception du message</li>
</ul></li>
<li>Récupérer des ressources :
<ul>
<li>S'il s'agit de ressources distantes :
<ul>
<li>Connexion à un service,</li>
<li><strong>Attente</strong> de l'acceptation de la connexion,</li>
<li>Envoi d'une requête,</li>
<li><strong>Attente</strong> que la réponse arrive,</li>
<li>Réception de la réponse,</li>
</ul></li>
<li>Si la ressource est protégée par un verrou ou un sémaphore :
<ul>
<li><strong>Attente</strong> de l'acquisition du verrou/sémaphore,</li>
<li>Récupération de la ressource,</li>
<li>Relâchement du verrou/sémaphore,</li>
</ul></li>
</ul></li>
<li>Combiner les ressources entre elles,</li>
<li>Répondre au client :
<ul>
<li><strong>Attente</strong> que le <em>medium</em> soit disponible en écriture,</li>
<li>Envoi de la réponse.</li>
</ul></li>
</ol>
<p>Comme vous le voyez, il est vraiment <em>très</em> courant d'attendre pour un serveur. Et il ne s'agit là que d'un schéma particulier dans une infinité d'applications possibles.</p>
<p>Ainsi, il serait possible d'optimiser de nombreux programmes en les rendant asynchrones, pour peu que l'on soit capable de rendre leurs <em>IO</em> <strong>non bloquantes</strong>, c'est-à-dire que l'on puisse laisser la main à d'autres tâches au lieu d'attendre, et reprendre celle-ci avec l'assurance que l'on pourra réaliser une <em>IO</em> immédiatement.</p>
<p>Il existe sous la plupart des systèmes d'exploitation une fonctionnalité qui permet de déterminer à un instant donné si un ou plusieurs flux de données sont accessibles en lecture ou en écriture. En fait, il en existe plein, mais nous allons nous concentrer sur celle qui sera disponible sur la plupart des systèmes d'exploitation : il s'agit de l'appel-système <a href="https://docs.python.org/3.4/library/select.html#select.select"><code>select()</code></a>.</p>
<p>Celui-ci, en Python, se présente sous la forme suivante :</p>
<pre class="sourceCode python"><code class="sourceCode python">select.select(rlist, wlist, xlist[, timeout])</code></pre>
<p>Où :</p>
<ul>
<li><code>rlist</code> est une liste de flux sur lesquels nous voulons lire des données,</li>
<li><code>wlist</code> est une liste de flux dans lesquels nous voulons écrire des données,</li>
<li><code>xlist</code> est une liste de flux que l'on surveille jusqu'à ce qu'il se produise des <em>conditions exceptionnelles</em> (ne nous attardons pas là-dessus),</li>
<li><code>timeout</code> est une durée (optionnelle) pendant laquelle on attend que des flux soient disponibles. Par défaut, on attend indéfiniment. Si on lui passe la valeur <code>0</code>, l'appel à <code>select()</code> retourne immédiatement.</li>
</ul>
<p>Cette fonction retourne trois listes :</p>
<ul>
<li>une contenant les flux disponibles en lecture à l'instant T,</li>
<li>une contenant les flux disponibles en écriture à l'instant T,</li>
<li>une autre contenant les flux victimes d'une exception.</li>
</ul>
<p>Que demander de plus ? Nous avons à notre disposition une fonction, dont l'exécution est instantannée, qui pourra nous permettre de réveiller les tâches en attente de lecture ou d'écriture.</p>
<p>On peut donc implémenter les coroutines d'attente asynchrones suivantes :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> select <span class="ch">import</span> select

<span class="co"># Attendre de façon asynchrone qu&#39;un flux soit disponible en lecture</span>
<span class="kw">def</span> wait_readable(stream):
    <span class="kw">while</span> <span class="ot">True</span>:
        rlist, _, _ = select([stream], [], [], <span class="dv">0</span>)
        <span class="kw">if</span> rlist:
            <span class="kw">return</span> stream
        <span class="kw">yield</span>

<span class="co"># Attendre de façon asynchrone qu&#39;un flux soit disponible en écriture</span>
<span class="kw">def</span> wait_writable(stream):
    <span class="kw">while</span> <span class="ot">True</span>:
        _, wlist, _ = select([], [stream], [], <span class="dv">0</span>)
        <span class="kw">if</span> wlist:
            <span class="kw">return</span> stream
        <span class="kw">yield</span></code></pre>
<p><strong>Note:</strong> Sous la plupart des systèmes d'exploitation Unix, vous pouvez utiliser <code>select()</code> pour attendre après n'importe quel flux de données (flux standard, fichiers, sockets), mais sous Windows, <em>seules</em> les sockets sont supportées.</p>
<p>Pour bien comprendre l'apport de ces deux fonctions, commençons par écrire un petit serveur qui se contente d'attendre une seconde avant de renvoyer les messages des clients :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> socket <span class="ch">import</span> socket
<span class="ch">from</span> time <span class="ch">import</span> sleep

<span class="kw">def</span> echo_server():
    <span class="co"># On crée une socket (par défaut : TCP/IP)</span>
    <span class="co"># qui écoutera sur le port 1234</span>
    sock = socket()
    sock.bind((<span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>))

    <span class="co"># On garde un maximum de 5 demandes de connexion en attente</span>
    sock.listen(<span class="dv">5</span>)
    <span class="kw">try</span>:
        <span class="kw">while</span> <span class="ot">True</span>:
            <span class="co"># Acceptation de la connection</span>
            conn, host = sock.accept()

            <span class="co"># Réception d&#39;un message (4 Mio max.)</span>
            msg = conn.recv(<span class="dv">4096</span>)
            <span class="dt">print</span>(<span class="st">&quot;message reçu de {!r}: {}&quot;</span>.<span class="dt">format</span>(host, msg.decode()))
            sleep(<span class="dv">1</span>)

            <span class="co"># Renvoi du message</span>
            conn.send(msg)
            conn.close()
    <span class="kw">finally</span>:
        sock.close()</code></pre>
<p>Lançons ce serveur dans une console.</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; echo_server()</code></pre>
<p>Dans <strong>une autre console</strong>, nous pouvons vérifier que celui-ci fonctionne en lui envoyant un message.</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; sock = socket.socket()
&gt;&gt;&gt; sock.<span class="ot">connect</span>((<span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>))
&gt;&gt;&gt; sock.send(<span class="st">&quot;Ohé !&quot;</span>.encode())
<span class="dv">6</span></code></pre>
<p>Le serveur affiche alors :</p>
<pre><code>message reçu de (&#39;127.0.0.1&#39;, 40568): Ohé !</code></pre>
<p>Nous n'avons plus qu'à récupérer sa réponse dans la fenêtre du client :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; data = sock.recv(<span class="dv">1024</span>)
&gt;&gt;&gt; data.decode()
<span class="co">&#39;Ohé !&#39;</span></code></pre>
<p>Parfait.</p>
<p>Pour corser les choses, essayons maintenant de lui envoyer 5 messages à la fois. Utilisons pour cela <code>asyncio</code>, ainsi que les deux coroutines d'attente que nous venons d'écrire :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> socket <span class="ch">import</span> socket
<span class="ch">from</span> datetime <span class="ch">import</span> datetime

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> echo(msg):
    <span class="co"># Connection TCP au port 1234</span>
    sock = socket()
    sock.<span class="ot">connect</span>((<span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>))

    <span class="co"># On attend de pouvoir envoyer le message</span>
    <span class="kw">yield</span> <span class="ch">from</span> wait_writable(sock)
    <span class="dt">print</span>(<span class="st">&quot;Envoi du message {!r}&quot;</span>.<span class="dt">format</span>(msg))
    sock.send(msg.encode())

    <span class="co"># On attend la réponse</span>
    <span class="kw">yield</span> <span class="ch">from</span> wait_readable(sock)
    data = sock.recv(<span class="dv">4096</span>)
    <span class="dt">print</span>(<span class="st">&quot;Message reçu:&quot;</span>, data.decode())
    sock.close()

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> echo_client():
    start = datetime.now()
    tasks = [echo(<span class="st">&quot;Hello {}&quot;</span>.<span class="dt">format</span>(num)) <span class="kw">for</span> num in <span class="dt">range</span>(<span class="dv">5</span>)]
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.wait(tasks)
    <span class="dt">print</span>(<span class="st">&quot;temps total:&quot;</span>, datetime.now() - start)</code></pre>
<p>Rien de fondamentalement compliqué : la coroutine <code>echo()</code> se contente de faire ce que nous avons réalisé juste avant dans la console, mais en attendant de façon asynchrone que la socket soit disponible en écriture, puis en lecture.</p>
<p>Voici le résultat :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(echo_client())
Envoi du message <span class="st">&#39;Hello 2&#39;</span>
Envoi du message <span class="st">&#39;Hello 1&#39;</span>
Envoi du message <span class="st">&#39;Hello 0&#39;</span>
Envoi du message <span class="st">&#39;Hello 3&#39;</span>
Envoi du message <span class="st">&#39;Hello 4&#39;</span>
Message reçu: Hello <span class="dv">2</span>
Message reçu: Hello <span class="dv">1</span>
Message reçu: Hello <span class="dv">0</span>
Message reçu: Hello <span class="dv">3</span>
Message reçu: Hello <span class="dv">4</span>
temps total: <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">05.013461</span></code></pre>
<p>À l'exécution, nous nous rendons compte que le serveur, qui est <em>synchrone</em>, traite les messages les uns à la suite des autres. Réimplémentons-le avec <code>asyncio</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> socket <span class="ch">import</span> socket
<span class="ch">import</span> asyncio

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> serve_echo(conn, host):
    <span class="co"># On suspend l&#39;exécution jusqu&#39;à recevoir un message</span>
    <span class="kw">yield</span> <span class="ch">from</span> wait_readable(conn)
    msg = conn.recv(<span class="dv">4096</span>)
    <span class="dt">print</span>(<span class="st">&quot;message reçu de {!r}: {}&quot;</span>.<span class="dt">format</span>(host, msg.decode()))
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">1</span>)

    <span class="co"># On suspend l&#39;exécution jusqu&#39;à pouvoir répondre au client</span>
    <span class="kw">yield</span> <span class="ch">from</span> wait_writable(conn)
    conn.send(msg)
    conn.close()

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> echo_server():
    <span class="co"># On crée une socket (par défaut : TCP/IP)</span>
    <span class="co"># qui écoutera sur le port 1234</span>
    sock = socket()
    sock.bind((<span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>))

    <span class="co"># On garde un maximum de 5 demandes de connexion en attente</span>
    sock.listen(<span class="dv">5</span>)
    <span class="kw">try</span>:
        <span class="kw">while</span> <span class="ot">True</span>:
            <span class="co"># On attent qu&#39;une demande de connexion arrive</span>
            <span class="kw">yield</span> <span class="ch">from</span> wait_readable(sock)

            <span class="co"># Acceptation de la connection</span>
            conn, host = sock.accept()

            <span class="co"># On programme le traitement de la requête dans une tâche séparée.</span>
            asyncio.async(serve_echo(conn, host))
    <span class="kw">finally</span>:
        sock.close()</code></pre>
<p>Lançons le serveur :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(echo_server())</code></pre>
<p>Puis le client :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop.run_until_complete(echo_client())
Envoi du message <span class="st">&#39;Hello 3&#39;</span>
Envoi du message <span class="st">&#39;Hello 0&#39;</span>
Envoi du message <span class="st">&#39;Hello 2&#39;</span>
Envoi du message <span class="st">&#39;Hello 1&#39;</span>
Envoi du message <span class="st">&#39;Hello 4&#39;</span>
Message reçu: Hello <span class="dv">3</span>
Message reçu: Hello <span class="dv">0</span>
Message reçu: Hello <span class="dv">2</span>
Message reçu: Hello <span class="dv">1</span>
Message reçu: Hello <span class="dv">4</span>
temps total: <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">01.001991</span></code></pre>
<p>Cette fois-ci, le serveur a traité toutes les requêtes en même temps, pour un temps d'exécution total de 1s au lieu de 5s.</p>
<p>Nous voici maintenant capables de réaliser des <em>IO</em> asynchrones ! Néanmoins, bien que nos coroutines <code>wait_readable()</code> et <code>wait_writable()</code> soient tout à fait fonctionnelles dans cet exemple, je vous <strong>déconseille très vivement</strong> de les utiliser. En effet, celles-ci bouclent à l'infini jusqu'à ce que le flux soit disponible, au lieu d'attendre passivement sans consommer de temps sur le processeur.</p>
<p>Rassurez-vous, <code>asyncio</code> propose de nombreuses façons de réaliser la même chose, et ce sans surcharger le processeur. Nous les examinerons dans les exemples suivants.</p>
<h1 id="exemple-n6-des-applications-réseau-avec-asyncio"><span class="header-section-number">7</span> Exemple n°6 : Des applications réseau avec <code>asyncio</code></h1>
<p>Le précédent exemple nous a fait toucher du doigt le concept d'<em>IO asynchrones</em>. Il est temps pour nous de nous familiariser avec les objets que nous propose <code>asyncio</code> pour réaliser de telles opérations sur un flux réseau.</p>
<p>Commençons par examiner un exemple que nous détaillerons ensuite. Voici le client du précédent exemple, réimplémenté avec les outils d'<code>asyncio</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> echo_client(message):
    reader, writer = <span class="kw">yield</span> <span class="ch">from</span> asyncio.open_connection(<span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>)

    <span class="dt">print</span>(<span class="st">&quot;Envoi du message :&quot;</span>, message)
    writer.write(message.encode())

    data = <span class="kw">yield</span> <span class="ch">from</span> reader.read(<span class="dv">1024</span>)
    <span class="dt">print</span>(<span class="st">&quot;Message reçu :&quot;</span>, data.decode())

    writer.close()</code></pre>
<p>Même sans savoir ce que sont ces objets <code>reader</code> et <code>writer</code>, la lecture de ce code est plutôt intuitive :</p>
<ul>
<li>La coroutine <code>ayncio.open_connection()</code> crée une connexion et retourne deux objets :
<ul>
<li>Un &quot;<em>reader</em>&quot; que l'on peut utiliser pour recevoir des données,</li>
<li>Un &quot;<em>writer</em>&quot; dont on se sert pour en envoyer,</li>
</ul></li>
<li>On se sert du <code>writer</code> pour envoyer un message au serveur (attention : la méthode <code>write()</code> est une fonction et non une coroutine ; pas de <code>yield from</code>),</li>
<li>On appelle ensuite, cette fois de façon asynchrone, la méthode <code>reader.read()</code> pour récupérer des données (on s'attend à moins de 1024 octets),</li>
<li>On ferme le <code>writer</code> pour clore la connexion.</li>
</ul>
<p>Avant d'aller plus loin, lançons le serveur de l'exemple précédent et vérifions que ce code fonctionne dans la console :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="ch">import</span> asyncio
&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(echo_client(<span class="st">&quot;Coucou!&quot;</span>))
Envoi du message : Coucou!
Message reçu : Coucou!</code></pre>
<p>Par de surprise, le code se comporte comme prévu. Attardons-nous un peu sur ces fameux objets qu'<code>asyncio</code> a créés pour nous. Il s'agit en fait d'instances de <a href="https://docs.python.org/3/library/asyncio-stream.html#streamreader"><code>asyncio.StreamReader</code></a> et <a href="https://docs.python.org/3/library/asyncio-stream.html#streamwriter"><code>asyncio.StreamWriter</code></a>.</p>
<p>Il s'agit de l'interface haut niveau d'asyncio pour manipuler une connexion de type &quot;<em>Stream</em>&quot; (typiquement, TCP). La même interface existe également pour les sockets Unix. Ces objets représentent un les deux sens d'un même flux réseau. Ainsi, vous avez grosso-modo 4 méthodes à retenir :</p>
<ul>
<li>La <em>coroutine</em> <code>StreamReader.read(size)</code> sert à recevoir un bloc de <code>size</code> octets de données maximum, de façon asynchrone ;</li>
<li>La <em>coroutine</em> <code>StreamReader.readline()</code> sert à recevoir une &quot;ligne&quot; de données, terminée par le caractère ASCII spécial <em>LINE_FEED</em> (<code>'\n'</code>) ;</li>
<li>La <em>fonction</em> <code>StreamWriter.write(data)</code> sert à envoyer des données (sous la forme d'un objet <code>bytes</code>) sur le flux. En fait, les données seront placées en attente dans un tampon (<em>buffer</em>) qui ne sera vidé qu'au prochain <code>yield</code>…</li>
<li>Ce qui nous amène à la <em>coroutine</em> <code>StreamWriter.drain()</code>, qui sert à vider explicitement le tampon du <em>writer</em> pour envoyer les données sur le flux.</li>
</ul>
<p>C'est plutôt simple, non ? Ces objets reposent en fait sur une autre API, un petit peu plus bas niveau, d'<code>asyncio</code>, qui modélise des <em>protocoles</em> que l'on utilise par-dessus un <em>transport</em> (TCP, UDP…). Nous ne parlerons pas de cette API dans cet exemple, mais si vous êtes curieux, <a href="https://docs.python.org/3/library/asyncio-protocol.html">sa documentation</a> est plutôt claire et détaillée.</p>
<p>Cette même interface peut d'ailleurs nous permettre de créer facilement un serveur, au moyen de la coroutine <code>asyncio.start_server()</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="co">#!/usr/bin/env python3</span>
<span class="ch">import</span> asyncio

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> handle_echo(reader, writer):
    data = <span class="kw">yield</span> <span class="ch">from</span> reader.read(<span class="dv">1024</span>)
    <span class="dt">print</span>(<span class="st">&quot;Message reçu :&quot;</span>, data.decode())
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">1</span>)
    writer.write(data)
    <span class="kw">yield</span> <span class="ch">from</span> writer.drain()
    writer.close()

<span class="kw">def</span> echo_server():
    loop = asyncio.get_event_loop()
    server = loop.run_until_complete(
        asyncio.start_server(handle_echo, <span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>)
    )

    <span class="kw">try</span>:
        loop.run_forever()
    <span class="kw">except</span> <span class="ot">KeyboardInterrupt</span>:
        <span class="kw">pass</span>

    server.close()
    loop.run_until_complete(server.wait_closed())
    loop.close()

<span class="kw">if</span> <span class="ot">__name__</span> == <span class="st">&#39;__main__&#39;</span>:
    echo_server()</code></pre>
<p>Ici, tout le code de notre serveur se trouve dans la coroutine <code>handle_echo</code>. Cette coroutine accepte un <code>StreamReader</code> et un <code>StreamWriter</code>, et ne retourne que lorsque l'échange avec un client est terminé.</p>
<p>Dans la fonction <code>echo_server()</code> on commence par créer un serveur en passant cette coroutine à <code>asyncio.start_server()</code> dans la boucle événementielle, puis on demande à la boucle de tourner &quot;à l'infini&quot; (jusqu'à ce que le processus soit interrompu).</p>
<p>Notez que nous aurions pu instancier le même service, mais sur des sockets Unix au lieu d'un flux TCP, simplement en utilisant la fonction <code>asyncio.start_unix_server()</code> à la place de <code>asyncio.start_server()</code> : la coroutine <code>handle_echo</code>, restera la même, et sera lancée dans une nouvelle tâche chaque fois qu'un client se connectera.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>En toute rigueur il s'agit d'un <em>générateur</em>, mais comme nous avons pu l'observer dans un <a href="https://zestedesavoir.com/articles/232/la-puissance-cachee-des-coroutines/">précédent article</a>, les générateurs de Python sont implémentés comme de véritables coroutines.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>D'ailleurs, le noyau de l'OS est justement là pour vous empêcher de toucher vous-même à l'ordonnanceur !<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Un <em>processus</em> peut être vu comme <strong>un programme en cours d'exécution</strong>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Ce comportement n'est pas exclusif aux processus : de la même façon, un <em>thread</em> <code>A</code> envoie au noyau un appel système lorsqu'il veut lancer un nouveau <em>thread</em> <code>B</code>.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>On peut par exemple lire un son sous Linux en <em>écrivant</em> des données dans un fichier spécial qui représente la carte son !<a href="#fnref5">↩</a></p></li>
</ol>
</section>
</body>
</html>
