<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Arnaud Calmettes (nohar)">
  <title>Découvrons la programmation asynchrone en quelques exemples</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" href="default.css">
  
  
</head>
<body>
<header>
<h1 class="title">Découvrons la programmation asynchrone en quelques exemples</h1>
<h2 class="author">Arnaud Calmettes (nohar)</h2>
</header>
<h1 id="ça-veut-dire-quoi-asynchrone"><span class="header-section-number">1</span> Ça veut dire quoi, <em>asynchrone</em> ?</h1>
<p>En un mot comme en cent, un programme qui fonctionne de façon <em>asychrone</em>, c'est un programme qui évite au maximum de passer du temps à <em>attendre sans rien faire</em>, et qui s'arrange pour <em>s'occuper autant que possible pendant qu'il attend</em>. Cette façon d'optimiser le temps d'attente est tout à fait naturelle pour nous. Par exemple, on peut s'en rendre compte en observant le travail d'un serveur qui monte votre commande dans un <em>fast food</em>.</p>
<p>De façon synchrone :</p>
<ul>
<li>Préparer le hamburger :
<ul>
<li>Demander le hamburger en cuisine.</li>
<li>Attendre le hamburger (1 minute).</li>
<li>Récupérer le hamburger et le poser sur le plateau.</li>
</ul></li>
<li>Préparer les frites :
<ul>
<li>Mettre des frites à chauffer.</li>
<li>Attendre que les frites soient cuites (2 minutes).</li>
<li>Récupérer des frites et les poser sur le plateau.</li>
</ul></li>
<li>Préparer la boisson :
<ul>
<li>Placer un gobelet dans la machine à soda.</li>
<li>Remplir le gobelet (30 secondes).</li>
<li>Récupérer le gobelet et le poser sur le plateau.</li>
</ul></li>
</ul>
<p>En gros, si notre employé de <em>fast food</em> était synchrone, il mettrait 3 minutes et 30 secondes pour monter votre commande.</p>
<p>Alors que de façon asynchrone :</p>
<ul>
<li>Demander le hamburger en cuisine.</li>
<li>Mettre les frites à chauffer.</li>
<li>Placer un gobelet dans la machine à soda et le mettre à remplir.</li>
<li>Après 30 secondes: Récupérer le gobelet et le poser sur le plateau.</li>
<li>Après 1 minute : Récupérer le hamburger et le poser sur le plateau.</li>
<li>Après 2 minutes : Récupérer les frites et les poser sur le plateau.</li>
</ul>
<p>En travaillant de façon asynchrone, notre employé de <em>fast food</em> monte maintenant votre commande en 2 minutes. Mais ça ne s'arrête pas là !</p>
<ul>
<li><strong>Une commande <code>A</code> est confiée à l'employé</strong></li>
<li>Demander le burger pour <code>A</code> en cuisine</li>
<li>Mettre les frites à chauffer.</li>
<li>Placer un gobelet dans la machine à soda pour <code>A</code>.</li>
<li>Après 30 secondes : Récupérer le gobelet de <code>A</code> et le poser sur son plateau</li>
<li><strong>Une nouvelle commande <code>B</code> est prise et confiée à l'employé</strong></li>
<li>Demander le burger pour <code>B</code> en cuisine</li>
<li>Placer un gobelet dans la machine à soda pour <code>B</code>.</li>
<li>Après 1 minute : Le burger de <code>A</code> est prêt, le poser sur son plateau.</li>
<li>La boisson de <code>B</code> est remplie, la poser sur son plateau.</li>
<li>Après 1 minute 40 : Le burger de <code>B</code> est prêt, le poser sur son plateau.</li>
<li>Après 2 minutes : Les frites sont prêtes, servir <code>A</code> et <code>B</code></li>
</ul>
<p>Toujours en 2 minutes, l'employé asynchrone vient cette fois de servir 2 clients. Si vous vous mettez à la place du client <code>B</code> qui aurait dû attendre que l'employé finisse de monter la commande de <code>A</code> avant de s'occuper de la sienne dans un schéma synchrone, celui-ci a été servi en 1 minute 30 au lieu d'attendre 6 minutes 30.</p>
<p>Pensez-y la prochaine fois que vous irez manger dans un fast-food, et observez les serveurs. Leur boulot vous semblera d'un coup beaucoup plus compliqué qu'il n'y paraît !</p>
<p>En informatique, il existe un type de tâche qui impose aux programmes d'attendre sans rien faire : ce sont les <em>entrées/sorties</em> (ou <em>IO</em>). Nous verrons dans cet article que la programmation asynchrone est une façon extrêmement puissante d'implémenter des programmes qui réalisent plus d'IO que de calcul (comme une application de messagerie instantanée, par exemple).</p>
<h1 id="exemple-n1-une-boucle-événementielle-cest-essentiel"><span class="header-section-number">2</span> Exemple n°1 : Une boucle événementielle, c'est essentiel</h1>
<p>La notion fondamentale autour de laquelle <code>asyncio</code> a été construite est celle de <em>coroutine</em>.</p>
<p>Une coroutine est une tâche qui peut décider de se suspendre elle-même au moyen du mot-clé <code>yield</code>, et attendre jusqu'à ce que le code qui la contrôle décide de lui rendre la main en <em>itérant</em> dessus.</p>
<p>On peut imaginer, par exemple, écrire la fonction suivante :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> tic_tac():
    <span class="dt">print</span>(<span class="st">&quot;Tic&quot;</span>)
    <span class="kw">yield</span>
    <span class="dt">print</span>(<span class="st">&quot;Tac&quot;</span>)
    <span class="kw">yield</span>
    <span class="kw">return</span> <span class="st">&quot;Boum!&quot;</span></code></pre>
<p>Cette fonction, puisqu'elle utilise le mot-clé <code>yield</code>, définit une <em>coroutine</em><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Si on l'invoque, la fonction <code>tic_tac</code> retourne une tâche prête à être exécutée, mais n'exécute pas les instructions qu'elle contient.</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; task = tic_tac()
&gt;&gt;&gt; task
&lt;generator <span class="dt">object</span> tic_tac at <span class="bn">0x7fe157023280</span>&gt;</code></pre>
<p>En termes de vocabulaire, on dira que notre fonction <code>tic_tac</code> est une <em>fonction coroutine</em>, c'est-à-dire une fonction qui <strong>construit une coroutine</strong>. La coroutine est contenue ici dans la variable <code>task</code>.</p>
<p>Nous pouvons maintenant exécuter son code jusqu'au prochain <code>yield</code>, en nous servant de la fonction standard <code>next()</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="dt">next</span>(task)
Tic
&gt;&gt;&gt; <span class="dt">next</span>(task)
Tac
&gt;&gt;&gt; <span class="dt">next</span>(task)
Traceback (most recent call last):
  File <span class="st">&quot;&lt;stdin&gt;&quot;</span>, line <span class="dv">1</span>, in &lt;module&gt;
<span class="ot">StopIteration</span>: Boum!</code></pre>
<p>Lorsque la tâche est terminée, une exception <code>StopIteration</code> est levée. Celle-ci contient la valeur de retour de la coroutine. Jusqu'ici, rien de bien sorcier. Dès lors, on peut imaginer créer une petite boucle pour exécuter cette coroutine jusqu'à épuisement :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; task = tic_tac()
&gt;&gt;&gt; <span class="kw">while</span> <span class="ot">True</span>:
...     <span class="kw">try</span>:
...         <span class="dt">next</span>(task)
...     <span class="kw">except</span> <span class="ot">StopIteration</span> <span class="ch">as</span> stop:
...         <span class="dt">print</span>(<span class="st">&quot;valeur de retour:&quot;</span>, <span class="dt">repr</span>(stop.value))
...         <span class="kw">break</span>
...
Tic
Tac
valeur de retour: <span class="st">&#39;Boum!&#39;</span></code></pre>
<p>Afin de nous affranchir de la sémantique des itérateurs de Python, créons une classe <code>Task</code> qui nous permettra de manipuler nos coroutines plus aisément :</p>
<pre class="sourceCode python"><code class="sourceCode python">STATUS_NEW = <span class="st">&#39;NEW&#39;</span>
STATUS_RUNNING = <span class="st">&#39;RUNNING&#39;</span>
STATUS_FINISHED = <span class="st">&#39;FINISHED&#39;</span>
STATUS_ERROR = <span class="st">&#39;ERROR&#39;</span>

<span class="kw">class</span> Task:
    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, coro):
        <span class="ot">self</span>.coro = coro  <span class="co"># Coroutine à exécuter</span>
        <span class="ot">self</span>.name = coro.<span class="ot">__name__</span>
        <span class="ot">self</span>.status = STATUS_NEW  <span class="co"># Statut de la tâche</span>
        <span class="ot">self</span>.return_value = <span class="ot">None</span>  <span class="co"># Valeur de retour de la coroutine</span>
        <span class="ot">self</span>.error_value = <span class="ot">None</span>  <span class="co"># Exception levée par la coroutine</span>

    <span class="co"># Exécute la tâche jusqu&#39;à la prochaine pause</span>
    <span class="kw">def</span> run(<span class="ot">self</span>):
        <span class="kw">try</span>:
            <span class="co"># On passe la tâche à l&#39;état RUNNING et on l&#39;exécute jusqu&#39;à</span>
            <span class="co"># la prochaine suspension de la coroutine.</span>
            <span class="ot">self</span>.status = STATUS_RUNNING
            <span class="dt">next</span>(<span class="ot">self</span>.coro)
        <span class="kw">except</span> <span class="ot">StopIteration</span> <span class="ch">as</span> err:
            <span class="co"># Si la coroutine se termine, la tâche passe à l&#39;état FINISHED</span>
            <span class="co"># et on récupère sa valeur de retour.</span>
            <span class="ot">self</span>.status = STATUS_FINISHED
            <span class="ot">self</span>.return_value = err.value
        <span class="kw">except</span> <span class="ot">Exception</span> <span class="ch">as</span> err:
            <span class="co"># Si une autre exception est levée durant l&#39;exécution de la</span>
            <span class="co"># coroutine, la tâche passe à l&#39;état ERROR, et on récupère</span>
            <span class="co"># l&#39;exception pour laisser l&#39;utilisateur la traiter.</span>
            <span class="ot">self</span>.status = STATUS_ERROR
            <span class="ot">self</span>.error_value = err

    <span class="kw">def</span> is_done(<span class="ot">self</span>):
        <span class="kw">return</span> <span class="ot">self</span>.status in {STATUS_FINISHED, STATUS_ERROR}

    <span class="kw">def</span> <span class="ot">__repr__</span>(<span class="ot">self</span>):
        result = <span class="st">&#39;&#39;</span>
        <span class="kw">if</span> <span class="ot">self</span>.is_done():
            result = <span class="st">&quot; ({!r})&quot;</span>.<span class="dt">format</span>(<span class="ot">self</span>.return_value or <span class="ot">self</span>.error_value)

        <span class="kw">return</span> <span class="st">&quot;&lt;Task &#39;{}&#39; [{}]{}&gt;&quot;</span>.<span class="dt">format</span>(<span class="ot">self</span>.name, <span class="ot">self</span>.status, result)</code></pre>
<p>Son fonctionnement est plutôt simple. Réimplémentons notre boucle en nous servant de cette classe :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; task = Task(tic_tac())
&gt;&gt;&gt; task
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [NEW]&gt;
&gt;&gt;&gt; <span class="kw">while</span> not task.is_done():
...     task.run()
...     <span class="dt">print</span>(task)
...
Tic
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [RUNNING]&gt;
Tac
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [RUNNING]&gt;
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;
&gt;&gt;&gt; task.return_value
<span class="co">&#39;Boom!&#39;</span></code></pre>
<p>Bien. Nous avons une classe qui nous permet de manipuler des tâches en cours d'exécution, ces tâches étant implémentées sous la forme de coroutines. Il ne nous reste plus qu'à trouver un moyen d'exécuter plusieurs coroutines de façon <strong>concurrente</strong>, c'est-à-dire en parallèle les unes des autres. En effet, tout l'intérêt de la programmation asynchrone est d'être capable d'occuper le programme pendant qu'une tâche donnée est en attente d'un événement.</p>
<p>Pour cela, il suffit de construire une <em>file d'attente</em> de tâches à exécuter. En Python, l'objet le plus pratique pour modéliser une file d'attente est la classe standard <code>collections.deque</code> (<em>double-ended queue</em>). Cette classe possède les mêmes méthodes que les listes, auxquelles viennent s'ajouter :</p>
<ul>
<li><code>appendleft()</code> pour ajouter un élément au tout début de la liste,</li>
<li><code>popleft()</code> pour retirer (et retourner) le premier élément de la liste.</li>
</ul>
<p>Ainsi, il suffit ajouter les éléments à une extrémité de la file (<code>append()</code>), et consommer ceux de l'autre extrémité (<code>popleft()</code>). On pourrait arguer qu'il est possible d'ajouter des éléments n'importe où dans une liste avec la méthode <code>insert()</code>, mais la classe <code>deque</code> est vraiment <em>faite pour</em> créer des files et des piles : ses opérations aux extrémités sont bien plus efficaces que la méthode <code>insert()</code>.</p>
<p>Essayons d'exécuter en parallèle deux instances de notre coroutine <code>tic_tac</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="ch">from</span> collections <span class="ch">import</span> deque
&gt;&gt;&gt; running_tasks = deque()
&gt;&gt;&gt; running_tasks.append(Task(tic_tac()))
&gt;&gt;&gt; running_tasks.append(Task(tic_tac()))
&gt;&gt;&gt; <span class="kw">while</span> running_tasks:
...     <span class="co"># On récupère une tâche en attente et on l&#39;exécute</span>
...     task = running_tasks.popleft()
...     task.run()
...     <span class="kw">if</span> task.is_done():
...         <span class="co"># Si la tâche est terminée, on l&#39;affiche</span>
...         <span class="dt">print</span>(task)
...     <span class="kw">else</span>:
...         <span class="co"># La tâche n&#39;est pas finie, on la replace au bout</span>
...         <span class="co"># de la file d&#39;attente</span>
...         running_tasks.append(task)
...
Tic
Tic
Tac
Tac
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;</code></pre>
<p>Voilà qui est intéressant : la sortie des deux coroutines est entremêlée ! Cela signifie que les deux tâches ont été exécutées simultanément, de façon <strong>concurrente</strong>.</p>
<p>Nous avons tout ce qu'il nous faut pour modéliser une boucle événementielle, c'est-à-dire une boucle qui s'occupe de programmer l'exécution et le réveil des tâches dont elle a la charge. Implémentons celle-ci dans la classe <code>Loop</code> suivante :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> collections <span class="ch">import</span> deque

<span class="kw">class</span> Loop:
    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>):
        <span class="ot">self</span>._running = deque()

    <span class="kw">def</span> _loop(<span class="ot">self</span>):
        task = <span class="ot">self</span>._running.popleft()
        task.run()
        <span class="kw">if</span> task.is_done():
            <span class="dt">print</span>(task)
            <span class="kw">return</span>
        <span class="ot">self</span>.schedule(task)

    <span class="kw">def</span> run_until_empty(<span class="ot">self</span>):
        <span class="kw">while</span> <span class="ot">self</span>._running:
            <span class="ot">self</span>._loop()

    <span class="kw">def</span> schedule(<span class="ot">self</span>, task):
        <span class="kw">if</span> not <span class="dt">isinstance</span>(task, Task):
            task = Task(task)
        <span class="ot">self</span>._running.append(task)
        <span class="kw">return</span> task</code></pre>
<p>Vérifions :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> spam():
...     <span class="dt">print</span>(<span class="st">&quot;Spam&quot;</span>)
...     <span class="kw">yield</span>
...     <span class="dt">print</span>(<span class="st">&quot;Eggs&quot;</span>)
...     <span class="kw">yield</span>
...     <span class="dt">print</span>(<span class="st">&quot;Bacon&quot;</span>)
...     <span class="kw">yield</span>
...     <span class="kw">return</span> <span class="st">&quot;SPAM!&quot;</span>
...
&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.schedule(tic_tac())
&gt;&gt;&gt; event_loop.schedule(spam())
&gt;&gt;&gt; event_loop.run_until_empty()
Tic
Spam
Tac
Eggs
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;
Bacon
&lt;Task <span class="st">&#39;spam&#39;</span> [FINISHED] (<span class="st">&#39;SPAM!&#39;</span>)&gt;</code></pre>
<p>Tout fonctionne parfaitement. Dotons tout de même notre classe <code>Loop</code> d'une dernière méthode pour exécuter la boucle jusqu'à épuisement d'une coroutine en particulier :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Loop:
    <span class="co"># ...</span>
    <span class="kw">def</span> run_until_complete(<span class="ot">self</span>, task):
        task = <span class="ot">self</span>.schedule(task)
        <span class="kw">while</span> not task.is_done():
            <span class="ot">self</span>._loop()</code></pre>
<p>Testons-la :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.run_until_complete(tic_tac())
Tic
Tac
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;</code></pre>
<p>Pas de surprise.</p>
<p>Toute la programmation asynchrone repose sur ce genre de boucle qui sert en fait d'<em>ordonnanceur</em> aux tâches en cours d'exécution. Pour vous en convaincre, regardez ce bout de code qui utilise <code>asyncio</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="ch">import</span> asyncio
&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(tic_tac())
Tic
Tac
<span class="co">&#39;Boom!&#39;</span>
&gt;&gt;&gt; loop.run_until_complete(asyncio.wait([tic_tac(), spam()]))
Spam
Tic
Eggs
Tac
Bacon
({Task(&lt;tic_tac&gt;)&lt;result=<span class="st">&#39;Boom!&#39;</span>&gt;, Task(&lt;spam&gt;)&lt;result=<span class="st">&#39;SPAM!&#39;</span>&gt;}, <span class="dt">set</span>())</code></pre>
<p>Drôlement familier, n'est-ce pas ? Ne bloquez pas sur la fonction <code>asyncio.wait</code> : il s'agit simplement d'une coroutine qui sert à lancer plusieurs tâches en parallèle et attendre que celles-ci se terminent avant de retourner. Nous la reprogrammerons nous-mêmes très bientôt. ;)</p>
<h1 id="exemple-n2-exécuter-des-tâches-concurrentes"><span class="header-section-number">3</span> Exemple n°2 : Exécuter des tâches concurrentes</h1>
<p>Dans le précédent exemple, nous avons déclenché l'exécution concurrente de deux coroutines en les programmant explicitement dans la boucle :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.schedule(Task(tic_tac()))
&gt;&gt;&gt; event_loop.schedule(Task(spam()))
&gt;&gt;&gt; event_loop.run_until_empty()
<span class="co"># les deux coroutines s&#39;exécutent en parallèle.</span></code></pre>
<p>Ce petit morceau de code nous a permis de prouver qu'avec une boucle événementielle et des coroutines, on avait tout ce qu'il nous fallait pour définir un modèle d'exécution concurrent.</p>
<p>Cependant, dans la pratique, on ne devrait pas avoir à manipuler directement la boucle pour réaliser quelque chose d'aussi simple que le lancement d'une tâche parallèle. En fait, <strong>nos classes <code>Loop</code> et <code>Task</code> sont l'implémentation à bas niveau d'un petit framework asynchrone</strong>, le programmeur ne devrait jamais avoir à y toucher à part pour lancer le programme asynchrone qu'il a écrit dans des coroutines.</p>
<p>Si l'on se réfère aux autres modes de concurrence :</p>
<ul>
<li>On peut lancer un <em>thread</em> et attendre la fin de l'exécution de celui-ci depuis n'importe quel <em>thread</em> en cours d'exécution.</li>
<li>On peut créer, attendre ou arrêter un processus depuis n'importe quel processus en cours d'exécution.</li>
</ul>
<p>Qu'il s'agisse de <em>threads</em> ou de processus, ces actions ne requièrent à aucun moment d'interagir directement avec l'ordonnanceur de votre système d'exploitation (OS).<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>De façon analogue, <strong>on devrait pouvoir lancer, attendre la fin de l'exécution, ou annuler une coroutine depuis n'importe quelle coroutine en cours d'exécution</strong>, sans avoir à toucher directement à la boucle événementielle.</p>
<p>Commençons par chercher le moyen de faire appel à une coroutine depuis une tâche en cours d'exécution. Rappelons d'abord que la syntaxe <code>yield from</code> introduite dans le langage depuis Python 3.3 nous permet déjà de passer la main à une autre coroutine. Par exemple, dans le code suivant, la coroutine <code>example</code> utilise cette syntaxe pour laisser temporairement la main à la coroutine <code>subtask</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> example():
    <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;example&#39;&quot;</span>)
    <span class="dt">print</span>(<span class="st">&quot;Lancement de la tâche &#39;subtask&#39;&quot;</span>)
    <span class="kw">yield</span> <span class="ch">from</span> subtask()
    <span class="dt">print</span>(<span class="st">&quot;Retour dans &#39;example&#39;&quot;</span>)
    <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">3</span>):
        <span class="dt">print</span>(<span class="st">&quot;(example)&quot;</span>)
        <span class="kw">yield</span>

<span class="kw">def</span> subtask():
    <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;subtask&#39;&quot;</span>)
    <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">2</span>):
        <span class="dt">print</span>(<span class="st">&quot;(subtask)&quot;</span>)
        <span class="kw">yield</span></code></pre>
<p>Vérifions :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.run_until_complete(example())
Tâche <span class="st">&#39;example&#39;</span>
Lancement de la tâche <span class="st">&#39;subtask&#39;</span>
Tâche <span class="st">&#39;subtask&#39;</span>
(subtask)
(subtask)
Retour dans <span class="st">&#39;example&#39;</span>
(example)
(example)
(example)
&lt;Task <span class="st">&#39;example&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>Ainsi, Python nous fournit déjà nativement un élément de syntaxe pour <em>lancer une tâche de façon séquentielle</em> à l'intérieur d'une coroutine. Mais ce n'est pas tout à fait ce que nous voulons : notre but, ici, est de réussir à lancer la coroutine <code>subtask</code> de façon qu'elle s'exécute <em>en parallèle</em> de la coroutine <code>example</code>, et non d'attendre que <code>subtask</code> soit terminée pour reprendre l'exécution de <code>example</code>.</p>
<p>Pour ce faire, nous allons tirer parti d'une particularité des coroutines que nous avons découverte <a href="https://zestedesavoir.com/articles/232/la-puissance-cachee-des-coroutines/">dans cet article</a> : il est possible d'échanger des messages avec une coroutine en cours d'exécution.</p>
<p>Pour bien comprendre ce qui va suivre, continuons, si vous le voulez bien, notre parallèle avec le fonctionnement d'un système d'exploitation. Lorsqu'un processus<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> <code>A</code> a besoin d'exécuter un programme <code>B</code> dans un processus concurrent, celui-ci réalise ce que l'on appelle un <em>appel système</em>, c'est-à-dire qu'il <em>envoie un message</em> à l'OS pour lui demander de bien vouloir lancer le nouveau programme <code>B</code>.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> Dans la pratique, cet <em>appel système</em> ressemble à s'y méprendre à n'importe quel appel de fonction. <strong>L'idée-clé, c'est que le processus peut <em>communiquer</em> avec le noyau du système d'exploitation en échangeant des <em>messages</em> avec lui.</strong></p>
<p>Nous allons maintenant transposer cette notion à notre cas : nous voulons réussir à faire communiquer la boucle événementielle avec les coroutines qu'elle exécute grâce à des <em>messages</em>.</p>
<p>Rappelons rapidement que l'on peut envoyer une donnée à une coroutine au moment où celle-ci se suspend, en utilisant la méthode <code>send</code> des coroutines :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> receiver():
...     <span class="kw">while</span> <span class="ot">True</span>:
...         data = <span class="kw">yield</span>
...         <span class="dt">print</span>(<span class="st">&#39;received:&#39;</span>, <span class="dt">repr</span>(data))
...
&gt;&gt;&gt; coro = receiver()
&gt;&gt;&gt; <span class="dt">next</span>(coro)  <span class="co"># La coroutine doit être démarrée pour recevoir des données</span>
&gt;&gt;&gt; coro.send(<span class="st">&quot;spam&quot;</span>)
received: <span class="st">&#39;spam&#39;</span>
&gt;&gt;&gt; coro.send(<span class="st">&quot;eggs&quot;</span>)
received: <span class="st">&#39;eggs&#39;</span>
&gt;&gt;&gt; coro.send(<span class="st">&quot;bacon&quot;</span>)
received: <span class="st">&#39;bacon&#39;</span></code></pre>
<p>À l'opposé, on peut également envoyer des données depuis une coroutine en passant un argument au mot-clé <code>yield</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> sender():
...     <span class="kw">while</span> <span class="ot">True</span>:
...        <span class="kw">yield</span> <span class="st">&quot;spam&quot;</span>
...        <span class="kw">yield</span> <span class="st">&quot;eggs&quot;</span>
...        <span class="kw">yield</span> <span class="st">&quot;bacon&quot;</span>
...
&gt;&gt;&gt; coro = sender()
&gt;&gt;&gt; coro.send(<span class="ot">None</span>)  <span class="co"># Équivalent à next(coro)</span>
<span class="co">&#39;spam&#39;</span>
&gt;&gt;&gt; coro.send(<span class="ot">None</span>)
<span class="co">&#39;eggs&#39;</span>
&gt;&gt;&gt; coro.send(<span class="ot">None</span>)
<span class="co">&#39;bacon&#39;</span></code></pre>
<p>On peut donc parfaitement imaginer simuler un appel système :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> requester():
...     data = <span class="kw">yield</span> <span class="st">&#39;REQUÊTE&#39;</span>
...     <span class="dt">print</span>(<span class="st">&#39;data:&#39;</span>, <span class="dt">repr</span>(data))
...     <span class="kw">return</span>
...
&gt;&gt;&gt; coro = requester()
&gt;&gt;&gt; coro.send(<span class="ot">None</span>)  <span class="co"># On lance la coroutine</span>
<span class="co">&#39;REQUÊTE&#39;</span></code></pre>
<p>La coroutine vient de nous envoyer un message. Répondons-lui.</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; coro.send(<span class="st">&#39;RÉPONSE&#39;</span>)
data: <span class="st">&#39;RÉPONSE&#39;</span></code></pre>
<p>Comme vous le constatez, la syntaxe de Python rend assez intuitif le fait que notre coroutine peut envoyer des requêtes à son environnement d'exécution, et se suspendre jusqu'à ce qu'on lui envoie le résultat de cette requête.</p>
<p>Pour que ces messages soient transmis par notre classe <code>Task</code> du premier exemple, nous devons l'accomoder un petit peu :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Task:
    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>):
        <span class="co"># ...</span>
        <span class="ot">self</span>.msg = <span class="ot">None</span>

    <span class="kw">def</span> run(<span class="ot">self</span>):
        <span class="kw">try</span>:
            <span class="ot">self</span>.status = STATUS_RUNNING
            <span class="kw">return</span> <span class="ot">self</span>.coro.send(<span class="ot">self</span>.msg)
        <span class="co"># ...</span></code></pre>
<p>Ainsi, on peut envoyer un message à une tâche simplement en affectant son attribut <code>task.msg</code>, et la méthode <code>task.run()</code> retourne maintenant tous les messages que la coroutine pourrait <code>yield</code>-er.</p>
<p>Reste à déterminer la forme des messages grâce auxquels la coroutine peut faire appel à sa boucle d'exécution. De manière similaire aux appels système, on peut par exemple décider que ces messages seront un tuple sous la forme suivante :</p>
<pre class="sourceCode python"><code class="sourceCode python">(TYPE_APPEL, arguments)</code></pre>
<p>Par exemple, pour demander à la boucle événementielle de lancer une ou plusieurs coroutines en parallèle, le message serait :</p>
<pre class="sourceCode python"><code class="sourceCode python">(<span class="st">&#39;SCHEDULE&#39;</span>, [task1, task2, ...])</code></pre>
<p>Modifions la méthode <code>_loop()</code> de notre boucle pour qu'elle réagisse à ces messages :</p>
<pre class="sourceCode python"><code class="sourceCode python">CALL_SCHEDULE = <span class="st">&#39;SCHEDULE&#39;</span>

<span class="kw">class</span> Loop:
    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>):
        <span class="ot">self</span>._running = deque()

    <span class="kw">def</span> _loop(<span class="ot">self</span>):
        task = <span class="ot">self</span>._running.popleft()
        msg = task.run()  <span class="co"># Réception du message</span>

        <span class="kw">if</span> task.is_done():
            <span class="dt">print</span>(task)
        <span class="kw">else</span>:
            <span class="ot">self</span>.schedule(task)

        <span class="kw">if</span> not msg:
            <span class="kw">return</span>

        msg_type, args = msg
        <span class="kw">if</span> msg_type == CALL_SCHEDULE:
            <span class="co"># Si le message est un ordre d&#39;exécuter des tâches parallèles,</span>
            <span class="co"># on programme ces tâches et on les retourne à la tâche</span>
            <span class="co"># appelante.</span>
            task.msg = <span class="dt">tuple</span>(<span class="ot">self</span>.schedule(subtask) <span class="kw">for</span> subtask in args)
        <span class="kw">else</span>:
            <span class="kw">raise</span> <span class="ot">RuntimeError</span>(<span class="st">&quot;Message inconnu : {}&quot;</span>.<span class="dt">format</span>(msg_type))</code></pre>
<p>On peut maintenant abstraire ce message derrière une coroutine que nous appellerons <code>ensure_future()</code>, pour respecter la même nomenclature qu'<code>asyncio</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> ensure_future(task):
    <span class="co"># L&#39;appel à CALL_SCHEDULE retourne un tuple à un seul élément dans ce cas</span>
    (task,) = <span class="kw">yield</span> CALL_SCHEDULE, [task]
    <span class="kw">return</span> task</code></pre>
<p>Nous disposons désormais de deux façons d'exécuter une sous-tâche depuis une coroutine :</p>
<ul>
<li><p><code>yield from subtask()</code> : lance l'exécution d'une coroutine de façon <em>séquentielle</em>, c'est-à-dire en lui laissant la main jusqu'à ce que celle-ci se soit terminée.</p></li>
<li><p><code>yield from ensure_future(subtask())</code> : lance l'exécution d'une coroutine <em>en parallèle</em>.</p></li>
</ul>
<p>Ainsi, si nous modifions notre coroutine <code>example()</code> définie plus haut en conséquence, nous pouvons vérifier que nos tâches sont bien exécutées de façon concurrente :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> example():
...     <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;example&#39;&quot;</span>)
...     <span class="dt">print</span>(<span class="st">&quot;Lancement de la tâche &#39;subtask&#39;&quot;</span>)
...     <span class="kw">yield</span> <span class="ch">from</span> ensure_future(subtask())
...     <span class="dt">print</span>(<span class="st">&quot;Retour dans &#39;example&#39;&quot;</span>)
...     <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">3</span>):
...         <span class="dt">print</span>(<span class="st">&quot;(example)&quot;</span>)
...         <span class="kw">yield</span>
...
&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.run_until_complete(example())
Tâche <span class="st">&#39;example&#39;</span>
Lancement de la tâche <span class="st">&#39;subtask&#39;</span>
Retour dans <span class="st">&#39;example&#39;</span>
(example)
Tâche <span class="st">&#39;subtask&#39;</span>
(subtask)
(example)
(subtask)
(example)
&lt;Task <span class="st">&#39;subtask&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
&lt;Task <span class="st">&#39;example&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>Magique, n'est-ce pas ?</p>
<p>Par contre, une fois que notre coroutine est lancée, nous n'avons pas tout à fait le contrôle de son exécution. Par exemple, si nous rendions la tâche <code>subtask</code> plus longue qu'<code>example</code>, celle-ci lui « survivrait » :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> subtask():
...     <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;subtask&#39;&quot;</span>)
...     <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">5</span>):
...         <span class="dt">print</span>(<span class="st">&quot;(subtask)&quot;</span>)
...         <span class="kw">yield</span>
...
&gt;&gt;&gt; event_loop.run_until_complete(example())
Tâche <span class="st">&#39;example&#39;</span>
Lancement de la tâche <span class="st">&#39;subtask&#39;</span>
Retour dans <span class="st">&#39;example&#39;</span>
(example)
Tâche <span class="st">&#39;subtask&#39;</span>
(subtask)
(example)
(subtask)
(example)
(subtask)
&lt;Task <span class="st">&#39;example&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>L'exécution s'arrête avec la fin de la coroutine <code>example</code>, mais la coroutine <code>subtask</code>, elle, n'a pas fini. Elle est encore suspendue dans la boucle, à l'état de zombie alors que le reste du programme est terminé. Vidons ce qu'il reste dans la boucle événementielle :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop.run_until_empty()
(subtask)
(subtask)
&lt;Task <span class="st">&#39;subtask&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p><strong>Que faire si nous ne voulons pas qu'une coroutine quitte avant une sous-tâche qu'elle aurait lancée en parallèle ?</strong></p>
<p>Nous avons deux solutions. La première, dont nous nous contenterons dans cet exemple, serait de pouvoir <em>annuler</em> une tâche en cours d'exécution. Il nous suffit pour cela de créer un nouvel état dans notre classe <code>Task</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">STATUS_CANCELLED = <span class="st">&quot;CANCELLED&quot;</span>

<span class="kw">class</span> Task:

    <span class="co"># ...</span>

    <span class="kw">def</span> cancel(<span class="ot">self</span>):
        <span class="kw">if</span> <span class="ot">self</span>.is_done():
            <span class="co"># Inutile d&#39;annuler une tâche déjà terminée</span>
            <span class="kw">return</span>
        <span class="ot">self</span>.status = STATUS_CANCELLED

    <span class="kw">def</span> is_cancelled(<span class="ot">self</span>):
        <span class="kw">return</span> <span class="ot">self</span>.status == STATUS_CANCELLED</code></pre>
<p>Rajoutons un test dans la boucle événementielle pour déprogrammer les tâches annulées :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Loop:

    <span class="co"># ...</span>

    <span class="kw">def</span> _loop(<span class="ot">self</span>):
        task = <span class="ot">self</span>._running.popleft()

        <span class="kw">if</span> task.is_cancelled():
            <span class="co"># Si la tâche a été annulée,</span>
            <span class="co"># on ne l&#39;exécute pas et on &quot;l&#39;oublie&quot;.</span>
            <span class="dt">print</span>(task)
            <span class="kw">return</span>

        <span class="co"># ... le reste de la méthode est identique</span></code></pre>
<p>Il ne nous reste plus qu'une petite coroutine utilitaire à écrire pour annuler une tâche en cours d'exécution :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> cancel(task):
    <span class="co"># On annule la tâche</span>
    task.cancel()
    <span class="co"># On laisse la main à la boucle événementielle pour qu&#39;elle ait l&#39;occasion</span>
    <span class="co"># de prendre en compte l&#39;annulation</span>
    <span class="kw">yield</span>

<span class="kw">def</span> example():
    <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;example&#39;&quot;</span>)
    <span class="dt">print</span>(<span class="st">&quot;Lancement de la tâche &#39;subtask&#39;&quot;</span>)
    sub = <span class="kw">yield</span> <span class="ch">from</span> ensure_future(subtask())
    <span class="dt">print</span>(<span class="st">&quot;Retour dans &#39;example&#39;&quot;</span>)
    <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">3</span>):
        <span class="dt">print</span>(<span class="st">&quot;(example)&quot;</span>)
        <span class="kw">yield</span>
    <span class="kw">yield</span> <span class="ch">from</span> cancel(sub)</code></pre>
<p>Vérifions :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop.run_until_complete(example())
Tâche <span class="st">&#39;example&#39;</span>
Lancement de la tâche <span class="st">&#39;subtask&#39;</span>
Retour dans <span class="st">&#39;example&#39;</span>
(example)
Tâche <span class="st">&#39;subtask&#39;</span>
(subtask)
(example)
(subtask)
(example)
(subtask)
&lt;Task <span class="st">&#39;subtask&#39;</span> [CANCELLED]&gt;
&lt;Task <span class="st">&#39;example&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>Notre mécanisme d'annulation fonctionne comme prévu. Cela dit, on peut aussi imaginer tout simplement vouloir <em>attendre</em> de façon asynchrone que la sous-tâche ait terminé son exécution avant de quitter proprement…</p>
<p>Mais ça, je vous le garde pour le prochain exemple. ;)</p>
<h1 id="exemple-n3-attendre-de-façon-asynchrone"><span class="header-section-number">4</span> Exemple n°3 : Attendre de façon asynchrone</h1>
<p>À la fin du précédent exemple, nous avons implémenté un petit mécanisme <em>d'annulation</em> de tâches en cours d'exécution, qui permet d'éviter qu'une tâche <em>fille</em> survive à sa tâche <em>mère</em> en s'exécutant plus longtemps qu'elle. Cependant, il n'est peut-être pas toujours adéquat d'annuler brutalement une tâche. Par exemple, on peut imaginer que si celle-ci a des ressources à libérer, l'annuler risque de créer une fuite de mémoire…</p>
<p>Pour cette raison, on veut se donner un moyen <em>d'attendre</em> (de façon asynchrone) qu'une tâche ait fini de s'exécuter. En soi, cela n'est pas bien difficile, puisqu'il suffit de <code>yield</code>-er <em>tant que</em> l'événement que nous attendons ne s'est pas encore produit :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> example():
    <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;example&#39;&quot;</span>)
    <span class="dt">print</span>(<span class="st">&quot;Lancement de la tâche &#39;subtask&#39;&quot;</span>)
    sub = <span class="kw">yield</span> <span class="ch">from</span> ensure_future(subtask())
    <span class="dt">print</span>(<span class="st">&quot;Retour dans &#39;example&#39;&quot;</span>)
    <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">3</span>):
        <span class="dt">print</span>(<span class="st">&quot;(example)&quot;</span>)
        <span class="kw">yield</span>

    <span class="dt">print</span>(<span class="st">&quot;En attente de la fin de &#39;subtask&#39;&quot;</span>)
    <span class="kw">while</span> not sub.is_done() and not sub.is_cancelled():
        <span class="kw">yield</span>
    <span class="dt">print</span>(<span class="st">&quot;Ça y est, je peux quitter&quot;</span>)

<span class="kw">def</span> subtask():
    <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;subtask&#39;&quot;</span>)
    <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">5</span>):
        <span class="dt">print</span>(<span class="st">&quot;(subtask)&quot;</span>)
        <span class="kw">yield</span></code></pre>
<p>Essayons :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.run_until_complete(example())
Tâche <span class="st">&#39;example&#39;</span>
Lancement de la tâche <span class="st">&#39;subtask&#39;</span>
Retour dans <span class="st">&#39;example&#39;</span>
(example)
Tâche <span class="st">&#39;subtask&#39;</span>
(subtask)
(example)
(subtask)
(example)
(subtask)
En attente de la fin de <span class="st">&#39;subtask&#39;</span>
(subtask)
(subtask)
&lt;Task <span class="st">&#39;subtask&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
Ça y est, je peux quitter
&lt;Task <span class="st">&#39;example&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>C'est plutôt enfantin, en fait.</p>
<p>Souvenez-vous maintenant de la coroutine <code>wait()</code> d'<code>asyncio</code> que nous avons aperçue dans l'exemple n°1 :</p>
<pre class="python3"><code>&gt;&gt;&gt; import asyncio
&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(asyncio.wait([tic_tac(), spam()]))
Tic
spam
Tac
eggs
bacon
({Task(&lt;tic_tac&gt;)&lt;result=&#39;Boum!&#39;&gt;, Task(&lt;spam&gt;)&lt;result=&#39;spam&#39;&gt;}, set())</code></pre>
<p>Celle-ci permet d'attendre qu'une ou plusieurs tâches (lancées en parallèle) soient terminées avant de rendre la main. Sa valeur de retour se compose de deux ensembles (<code>set()</code>) :</p>
<ul>
<li>Le premier contient les tâches qui se sont terminées normalement ;</li>
<li>Le second contient les tâches qui ont été annulées ou ont quitté sur une erreur.</li>
</ul>
<p>C'est cette fonction que nous allons implémenter maintenant, parce qu'il serait vraiment dommage de se passer de sa souplesse d'utilisation !</p>
<p>En fait, le plus difficile dans cette fonction est surtout sa partie cosmétique. Pour avoir un comportement souple, il faut gérer le cas où les tâches passées à cette fonction sont des coroutines qui n'ont pas encore été lancées, ou bien des instances de la classe <code>Task</code> que la boucle événementielle aurait préalablement créées pour programmer leur exécution.</p>
<p>Voilà ce que cela peut donner :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> wait(tasks):
    <span class="co"># On commence par séparer les tâches en cours d&#39;exécution des autres</span>
    running, to_launch, finished, error = <span class="dt">set</span>(), <span class="dt">set</span>(), <span class="dt">set</span>(), <span class="dt">set</span>()
    <span class="kw">for</span> task in tasks:
        <span class="kw">if</span> <span class="dt">isinstance</span>(task, Task):
            <span class="kw">if</span> task.status == STATUS_FINISHED:
                finished.add(task)
            <span class="kw">elif</span> task.status in {STATUS_ERROR, STATUS_CANCELLED}:
                error.add(task)
            <span class="kw">else</span>:
                running.add(task)
        <span class="kw">else</span>:
            to_launch.add(task)

    <span class="co"># On lance les tâches qui ont besoin d&#39;être lancées</span>
    <span class="kw">if</span> to_launch:
        launched = <span class="kw">yield</span> CALL_SCHEDULE, to_launch
        running.update(launched)

    <span class="co"># On attend que tout le monde ait fini de s&#39;exécuter</span>
    <span class="kw">while</span> running:
        <span class="co"># On itère sur une copie immuable de l&#39;ensemble &#39;running&#39;</span>
        <span class="co"># de façon à pouvoir modifier celui-ci sans risque dans la boucle</span>
        <span class="kw">for</span> task in <span class="dt">tuple</span>(running):
            <span class="kw">if</span> not (task.is_done() or task.is_cancelled()):
                <span class="kw">continue</span>
            running.remove(task)
            <span class="kw">if</span> task.status == STATUS_FINISHED:
                finished.add(task)
            <span class="kw">else</span>:
                error.add(task)

        <span class="co"># S&#39;il y a encore des tâches inachevées, on suspend la coroutine</span>
        <span class="kw">if</span> running:
            <span class="kw">yield</span>

    <span class="kw">return</span> finished, error</code></pre>
<p>Vérifions que nous avons bien un comportement similaire à <code>asyncio</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop = Loop()
&gt;&gt;&gt; event_loop.run_until_complete(wait([tic_tac(), spam()]))
Tic
Spam
Tac
Eggs
&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;
Bacon
&lt;Task <span class="st">&#39;spam&#39;</span> [FINISHED] (<span class="st">&#39;SPAM!&#39;</span>)&gt;
&lt;Task <span class="st">&#39;wait&#39;</span> [FINISHED] (({&lt;Task <span class="st">&#39;tic_tac&#39;</span> [FINISHED] (<span class="st">&#39;Boom!&#39;</span>)&gt;, &lt;Task <span class="st">&#39;spam&#39;</span> [FINISHED] (<span class="st">&#39;SPAM!&#39;</span>)&gt;}, <span class="dt">set</span>()))&gt;</code></pre>
<p>Pas mal ! Et pour attendre une tâche préalablement démarrée ?</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> example():
...     <span class="dt">print</span>(<span class="st">&quot;Tâche &#39;example&#39;&quot;</span>)
...     <span class="dt">print</span>(<span class="st">&quot;Lancement de la tâche &#39;subtask&#39;&quot;</span>)
...     sub = <span class="kw">yield</span> <span class="ch">from</span> ensure_future(subtask())
...     <span class="dt">print</span>(<span class="st">&quot;Retour dans &#39;example&#39;&quot;</span>)
...     <span class="kw">for</span> _ in <span class="dt">range</span>(<span class="dv">3</span>):
...         <span class="dt">print</span>(<span class="st">&quot;(example)&quot;</span>)
...         <span class="kw">yield</span>
...
...     <span class="dt">print</span>(<span class="st">&quot;En attente de la fin de &#39;subtask&#39;&quot;</span>)
...     <span class="kw">yield</span> <span class="ch">from</span> wait([sub])
...     <span class="dt">print</span>(<span class="st">&quot;Ça y est, je peux quitter&quot;</span>)
...
&gt;&gt;&gt; event_loop.run_until_complete(example())
Tâche <span class="st">&#39;example&#39;</span>
Lancement de la tâche <span class="st">&#39;subtask&#39;</span>
Retour dans <span class="st">&#39;example&#39;</span>
(example)
Tâche <span class="st">&#39;subtask&#39;</span>
(subtask)
(example)
(subtask)
(example)
(subtask)
En attente de la fin de <span class="st">&#39;subtask&#39;</span>
(subtask)
(subtask)
&lt;Task <span class="st">&#39;subtask&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
Ça y est, je peux quitter
&lt;Task <span class="st">&#39;example&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>Parfait. Nous venons d'implémenter notre première coroutine <em>d'attente asynchrone</em>. Celle-ci permet d'endormir une tâche pour ne la réveiller que lorsqu'une ou plusieurs autres tâches auront fini de s'exécuter. S'il y avait une seule chose à retenir de la programmation asynchrone, c'est bien ce mécanisme. C'est dans le fait de pouvoir dire, de façon tout à fait explicite : « je n'ai rien à faire pour le moment, réveillez-moi quand il se sera passé quelque chose d'intéressant », que réside tout l'intérêt de ce modèle d'exécution. Et ce genre d'attente est vraiment très fréquent en informatique !</p>
<p>Dans cet article, nous allons nous contenter de <em>simuler</em> ces tâches. Pour ce faire, il suffit de nous doter d'une coroutine d'endormissement que nous appellerons <code>async_sleep</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Nous utilisons les classes datetime et timedelta de la bibliothèque standard</span>
<span class="co"># La première représente une date (précise), la seconde une durée.</span>
<span class="ch">from</span> datetime <span class="ch">import</span> datetime, timedelta

<span class="kw">def</span> async_sleep(secs):

    <span class="co"># On calcule l&#39;heure à laquelle on doit se réveiller</span>
    wakeup = datetime.now() + timedelta(seconds=secs)

    <span class="co"># On laisse la main tant que l&#39;heure de réveil n&#39;est pas passée</span>
    <span class="kw">while</span> datetime.now() &lt; wakeup:
        <span class="kw">yield</span></code></pre>
<p>Vérifions rapidement qu'elle fonctionne :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="kw">def</span> sleep_test(secs, msg):
...     <span class="kw">yield</span> <span class="ch">from</span> async_sleep(secs)
...     <span class="dt">print</span>(msg)
...
&gt;&gt;&gt; event_loop.run_until_complete(
...     wait([
...         sleep_test(<span class="dv">3</span>, <span class="st">&#39;trois&#39;</span>),
...         sleep_test(<span class="dv">1</span>, <span class="st">&#39;un&#39;</span>),
...         sleep_test(<span class="dv">2</span>, <span class="st">&#39;deux&#39;</span>)
...     ])
... )
un
&lt;Task <span class="st">&#39;sleep_test&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
deux
&lt;Task <span class="st">&#39;sleep_test&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
trois
&lt;Task <span class="st">&#39;sleep_test&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
&lt;Task <span class="st">&#39;wait&#39;</span> [FINISHED] (...)&gt;</code></pre>
<p>Bien, nous avons maintenant tout ce qu'il nous faut pour modéliser un système asynchrone, comme notre employé de <em>fast food</em>, par exemple. Pour ramener cet exemple à des durées plus aisées à vérifier dans un programme, nous prendrons les temps suivants :</p>
<ul>
<li>Préparation d'un soda : 1 seconde,</li>
<li>Préparation d'un hamburger : 3 secondes,</li>
<li>Préparation d'une portion de frites : 4 secondes.</li>
</ul>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> get_soda():
    <span class="dt">print</span>(<span class="st">&quot;Remplissage du gobelet de soda&quot;</span>)
    <span class="kw">yield</span> <span class="ch">from</span> async_sleep(<span class="dv">1</span>)
    <span class="dt">print</span>(<span class="st">&quot;Le soda est prêt&quot;</span>)

<span class="kw">def</span> get_fries():
    <span class="dt">print</span>(<span class="st">&quot;Démarrage de la cuisson des frites&quot;</span>)
    <span class="kw">yield</span> <span class="ch">from</span> async_sleep(<span class="dv">4</span>)
    <span class="dt">print</span>(<span class="st">&quot;Les frites sont prêtes&quot;</span>)

<span class="kw">def</span> get_burger():
    <span class="dt">print</span>(<span class="st">&quot;Commande du burger en cuisine&quot;</span>)
    <span class="kw">yield</span> <span class="ch">from</span> async_sleep(<span class="dv">3</span>)
    <span class="dt">print</span>(<span class="st">&quot;Le burger est prêt&quot;</span>)</code></pre>
<p>Nous n'avons plus qu'à modéliser notre serveur. Commençons par le concevoir de façon séquentielle :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> serve():
    start = datetime.now()
    <span class="kw">yield</span> <span class="ch">from</span> get_soda()
    <span class="kw">yield</span> <span class="ch">from</span> get_burger()
    <span class="kw">yield</span> <span class="ch">from</span> get_fries()
    <span class="dt">print</span>(<span class="st">&quot;Client servi en&quot;</span>, datetime.now() - start)</code></pre>
<p>Lorsqu'il attend &quot;bêtement&quot; que tout soit prêt, le serveur met 8 secondes à servir un client.</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop.run_until_complete(serve())
Remplissage du gobelet de soda
Le soda est prêt
Commande du burger en cuisine
Le burger est prêt
Démarrage de la cuisson des frites
Les frites sont prêtes
Client servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.000307</span>
&lt;Task <span class="st">&#39;serve&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>Alors que si nous le modélisons de façon asynchrone…</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> async_serve():
    start = datetime.now()
    <span class="kw">yield</span> <span class="ch">from</span> wait([
        get_soda(),
        get_burger(),
        get_fries()
    ])
    <span class="dt">print</span>(<span class="st">&quot;Client servi en&quot;</span>, datetime.now() - start)</code></pre>
<p>… Le client est servi deux fois plus rapidement :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; event_loop.run_until_complete(async_serve())
Commande du burger en cuisine
Démarrage de la cuisson des frites
Remplissage du gobelet de soda
Le soda est prêt
&lt;Task <span class="st">&#39;get_soda&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
Le burger est prêt
&lt;Task <span class="st">&#39;get_burger&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
Les frites sont prêtes
&lt;Task <span class="st">&#39;get_fries&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;
Client servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.000214</span>
&lt;Task <span class="st">&#39;async_serve&#39;</span> [FINISHED] (<span class="ot">None</span>)&gt;</code></pre>
<p>En modélisant cet exemple du début, nous venons de <em>construire</em> l'ensemble des outils et des primitives nécessaires à la programmation asynchrone. Cela dit, ce n'est encore que le début du voyage. Dans le prochain exemple, nous allons nous servir d'<code>asyncio</code> pour modéliser ce système d'une façon un peu plus réaliste, et découvrir que ce serveur de <em>fast food</em> représente un véritable problème d'optimisation d'un serveur asynchrone !</p>
<h1 id="exemple-n4-modélisons-le-serveur-du-fast-food-avec-asyncio"><span class="header-section-number">5</span> Exemple n°4 : Modélisons le serveur du <em>fast food</em> avec <code>asyncio</code></h1>
<p>Maintenant que nous avons fait le tour de toutes les primitives qui permettent la programmation asynchrone, il est temps pour nous d'étudier un système asynchrone programmé avec <code>asyncio</code>. Afin d'éviter d'alourdir le propos dans cet exemples, nous nous contenterons de notre exemple <em>fil rouge</em> : l'employé de fast food, en nous promettant d'aborder des applications réseau réelles dans un article ultérieur.</p>
<p>Commençons par implémenter celui-ci avec <code>asyncio</code>.</p>
<p>En réalité, vous n'allez pas tellement être dépaysés puisque le framework standard reprend plus ou moins la même API que celle que nous avons développé dans les trois derniers exemples. Les seules différences sont que :</p>
<ul>
<li>Toutes les coroutines doivent être décorées par <code>@asyncio.coroutine</code>, ce qui permet notamment de créer des coroutines qui ne <code>yield</code>-ent jamais.</li>
<li><code>wait()</code> devient <a href="https://docs.python.org/3/library/asyncio-task.html#asyncio.wait"><code>asyncio.wait()</code></a>.</li>
<li><code>async_sleep()</code> devient <a href="https://docs.python.org/3/library/asyncio-task.html#asyncio.sleep"><code>asyncio.sleep()</code></a>.</li>
<li>notre coroutine <code>ensure_future()</code> devient la <strong>fonction</strong> <a href="https://docs.python.org/3/library/asyncio-task.html#asyncio.ensure_future"><code>asyncio.ensure_future()</code></a>. Notez toutefois que cette fonction n'existe que depuis Python 3.4.4. Dans les versions antérieures, celle-ci s'appelle <code>asyncio.async()</code>, mais son nom a été déprécié au profit de <code>ensure_future()</code> afin de libérer le mot-clé <code>async</code> pour Python 3.5.</li>
</ul>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">import</span> asyncio
<span class="ch">from</span> datetime <span class="ch">import</span> datetime

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_soda(client):
    <span class="dt">print</span>(<span class="st">&quot;  &gt; Remplissage du soda pour {}&quot;</span>.<span class="dt">format</span>(client))
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">1</span>)
    <span class="dt">print</span>(<span class="st">&quot;  &lt; Le soda de {} est prêt&quot;</span>.<span class="dt">format</span>(client))

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_fries(client):
    <span class="dt">print</span>(<span class="st">&quot;    &gt; Démarrage de la cuisson des frites pour {}&quot;</span>.<span class="dt">format</span>(client))
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">4</span>)
    <span class="dt">print</span>(<span class="st">&quot;    &lt; Les frites de {} sont prêtes&quot;</span>.<span class="dt">format</span>(client))

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_burger(client):
    <span class="dt">print</span>(<span class="st">&quot;    &gt; Commande du burger en cuisine pour {}&quot;</span>.<span class="dt">format</span>(client))
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">3</span>)
    <span class="dt">print</span>(<span class="st">&quot;    &lt; Le burger de {} est prêt&quot;</span>.<span class="dt">format</span>(client))

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> serve(client):
    <span class="dt">print</span>(<span class="st">&quot;=&gt; Commande passée par {}&quot;</span>.<span class="dt">format</span>(client))
    start_time = datetime.now()
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.wait(
        [
            get_soda(client),
            get_fries(client),
            get_burger(client)
        ]
    )
    total = datetime.now() - start_time
    <span class="dt">print</span>(<span class="st">&quot;&lt;= {} servi en {}&quot;</span>.<span class="dt">format</span>(client, datetime.now() - start_time))</code></pre>
<p>Rien de franchement dépaysant. Pour exécuter ce code, là aussi l'API est sensiblement la même que notre classe <code>Loop</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(serve(<span class="st">&quot;A&quot;</span>))
=&gt; Commande passée par A
    &gt; Remplissage du soda pour A
    &gt; Commande du burger en cuisine pour A
    &gt; Démarrage de la cuisson des frites pour A
    &lt; Le soda de A est prêt
    &lt; Le burger de A est prêt
    &lt; Les frites de A sont prêtes
&lt;= A servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.003105</span></code></pre>
<p>Pas d'erreur de syntaxe, le code fonctionne. On peut commencer à travailler.</p>
<p>Remarquons dans un premier temps que notre serveur <strong>manque de réalisme</strong>. En effet, si nous lui demandons de servir deux clients en même temps, voilà ce qui se produit :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop.run_until_complete(
...     asyncio.wait([serve(<span class="st">&quot;A&quot;</span>), serve(<span class="st">&quot;B&quot;</span>)])
... )
=&gt; Commande passée par A
=&gt; Commande passée par B
    &gt; Remplissage du soda pour A
    &gt; Commande du burger en cuisine pour A
    &gt; Démarrage de la cuisson des frites pour A
    &gt; Démarrage de la cuisson des frites pour B
    &gt; Remplissage du soda pour B
    &gt; Commande du burger en cuisine pour B
    &lt; Le soda de A est prêt
    &lt; Le soda de B est prêt
    &lt; Le burger de A est prêt
    &lt; Le burger de B est prêt
    &lt; Les frites de A sont prêtes
    &lt; Les frites de B sont prêtes
&lt;= A servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.002609</span>
&lt;= B servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.002792</span></code></pre>
<p>Les deux commandes ont été servies simultanément, de la même façon. La préparation des trois ingrédients s'est chevauchée, comme s'il était possible de faire couler une infinité de sodas, de cuire une infinité de frites <em>à la demande</em> pour les clients, et de préparer une infinité de hamburgers en parallèle.</p>
<p>En bref : <strong>notre modélisation manque de contraintes</strong>.</p>
<p>Pour améliorer ce programme, nous allons modéliser les contraintes suivantes :</p>
<ul>
<li>La machine à sodas ne peut faire couler <strong>qu'un seul soda à la fois</strong>. Dans une application réelle, cela reviendrait à <em>requêter un service synchrone qui ne supporte pas les accès concurrents</em> ;</li>
<li>Il n'y a que 3 cuisiniers dans le restaurant, donc <strong>on ne peut pas préparer plus de trois hamburgers en même temps</strong>. Dans la réalité, cela revient à <em>requêter un service synchrone dont trois instances tournent en parallèle</em> ;</li>
<li>Le bac à frites s'utilise en faisant cuire 5 portions de frites d'un coup, pour servir ensuite 5 clients instantanément. Dans la réalité, cela revient, à peu de choses près, à <em>simuler un service synchrone qui fonctionne avec un cache</em>.</li>
</ul>
<p>La machine à soda est certainement la plus simple. Il est possible de verrouiller une ressource de manière à ce qu'une seule tâche puisse y accéder à la fois, en utilisant ce que l'on appelle un <strong>verrou</strong> (<code>asyncio.Lock</code>). Plaçons un verrou sur notre machine à soda :</p>
<pre class="sourceCode python"><code class="sourceCode python">SODA_LOCK = asyncio.Lock()

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_soda(client):
    <span class="co"># Acquisition du verrou</span>
    <span class="kw">with</span> (<span class="kw">yield</span> <span class="ch">from</span> SODA_LOCK):
        <span class="co"># Une seule tâche à la fois peut exécuter ce bloc</span>
        <span class="dt">print</span>(<span class="st">&quot;    &gt; Remplissage du soda pour {}&quot;</span>.<span class="dt">format</span>(client))
        <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">1</span>)
        <span class="dt">print</span>(<span class="st">&quot;    &lt; Le soda de {} est prêt&quot;</span>.<span class="dt">format</span>(client))</code></pre>
<p>Le <code>with (yield from SODA_LOCK)</code> signifie que lorsque le serveur arrive à la machine à soda pour y déposer un gobelet :</p>
<ul>
<li>soit la machine est libre (déverrouillée), auquel cas il peut la verrouiller pour l'utiliser immédiatement,</li>
<li>soit celle-ci est déjà en train de fonctionner, auquel cas il attend que le soda en cours de préparation soit prêt avant de verrouiller la machine à son tour.</li>
</ul>
<p>Passons à la cuisine. Seuls 3 burgers peuvent être fabriqués en même temps. Cela peut se modéliser en utilisant un <strong>sémaphore</strong> (<code>asyncio.Semaphore</code>), qui est une sorte de &quot;verrou multiple&quot;. On l'utilise pour qu'au plus N tâches puissent exécuter un morceau de code à un instant donné.</p>
<pre class="sourceCode python"><code class="sourceCode python">BURGER_SEM = asyncio.Semaphore(<span class="dv">3</span>)

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_burger(client):
    <span class="dt">print</span>(<span class="st">&quot;    &gt; Commande du burger en cuisine pour {}&quot;</span>.<span class="dt">format</span>(client))
    <span class="kw">with</span> (<span class="kw">yield</span> <span class="ch">from</span> BURGER_SEM):
        <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">3</span>)
        <span class="dt">print</span>(<span class="st">&quot;    &lt; Le burger de {} est prêt&quot;</span>.<span class="dt">format</span>(client))</code></pre>
<p>Le <code>with (yield from BURGER_SEM)</code> veut dire que lorsqu'une commande est passée en cuisine :</p>
<ul>
<li>soit il y a un cuisinier libre, et celui-ci commence immédiatement à préparer le hamburger,</li>
<li>soit tous les cuisiniers sont occupés, auquel cas on attend qu'il y en ait un qui se libère pour s'occuper de notre hamburger.</li>
</ul>
<p>Passons enfin au bac à frites. Cette fois, <code>asyncio</code> ne nous fournira pas d'objet magique, donc il va nous falloir réfléchir un peu plus. Il faut que l'on puisse l'utiliser <em>une fois</em> pour faire les frites des 5 prochaines commandes. Dans ce cas, un compteur semble une bonne idée :</p>
<ul>
<li>Chaque fois que l'on prend une portion de frites, on décrémente le compteur ;</li>
<li>S'il n'y a plus de frites dans le bac, il faut en refaire.</li>
</ul>
<p>Mais attention, si les frites sont déjà en cours de préparation, il est inutile de lancer une nouvelle fournée !</p>
<p>Voici comment on pourrait s'y prendre :</p>
<pre class="sourceCode python"><code class="sourceCode python">FRIES_COUNTER = <span class="dv">0</span>
FRIES_LOCK = asyncio.Lock()

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> get_fries(client):
    <span class="kw">global</span> FRIES_COUNTER
    <span class="kw">with</span> (<span class="kw">yield</span> <span class="ch">from</span> FRIES_LOCK):
        <span class="dt">print</span>(<span class="st">&quot;    &gt; Récupération des frites pour {}&quot;</span>.<span class="dt">format</span>(client))
        <span class="kw">if</span> FRIES_COUNTER == <span class="dv">0</span>:
            <span class="dt">print</span>(<span class="st">&quot;   ** Démarrage de la cuisson des frites&quot;</span>)
            <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">4</span>)
            FRIES_COUNTER = <span class="dv">5</span>
            <span class="dt">print</span>(<span class="st">&quot;   ** Les frites sont cuites&quot;</span>)
        FRIES_COUNTER -= <span class="dv">1</span>
        <span class="dt">print</span>(<span class="st">&quot;    &lt; Les frites de {} sont prêtes&quot;</span>.<span class="dt">format</span>(client))</code></pre>
<p>Dans cet exemple, on place un verrou sur le bac à frites pour qu'un seul serveur puisse y accéder à la fois. Lorsqu'un serveur arrive devant le bac à frites, soit celui-ci contient encore des portions de frites, auquel cas il en récupère une et retourne immédiatement, soit le bac est vide, donc le serveur met des frites à cuire avant de pouvoir en récupérer une portion.</p>
<p>À l'exécution :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop.run_until_complete(asyncio.wait([serve(<span class="st">&#39;A&#39;</span>), serve(<span class="st">&#39;B&#39;</span>)]))
=&gt; Commande passée par B
=&gt; Commande passée par A
    &gt; Remplissage du soda pour B
    &gt; Récupération des frites pour B
   ** Démarrage de la cuisson des frites
    &gt; Commande du burger en cuisine pour B
    &gt; Commande du burger en cuisine pour A
    &lt; Le soda de B est prêt
    &gt; Remplissage du soda pour A
    &lt; Le soda de A est prêt
    &lt; Le burger de B est prêt
    &lt; Le burger de A est prêt
   ** Les frites sont cuites
    &lt; Les frites de B sont prêtes
    &gt; Récupération des frites pour A
    &lt; Les frites de A sont prêtes
&lt;= B servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.003111</span>
&lt;= A servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.003093</span></code></pre>
<p>Nos deux tâches prennent toujours le même temps à s'exécuter, mais s'arrangent pour ne pas accéder simultanément à la machine à sodas ni au bac à frites.</p>
<p>Voyons maintenant ce que cela donne si 10 clients passent commande en même temps :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop.run_until_complete(
...     asyncio.wait([serve(clt) <span class="kw">for</span> clt in <span class="st">&#39;ABCDEFGHIJ&#39;</span>])
... )
...
<span class="co"># ... sortie filtrée ...</span>
&lt;= C servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.004512</span>
&lt;= D servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.004378</span>
&lt;= E servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.004262</span>
&lt;= F servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">06.008072</span>
&lt;= A servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">06.008074</span>
&lt;= G servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">08.006399</span>
&lt;= H servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">09.009187</span>
&lt;= B servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">09.009118</span>
&lt;= I servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">09.015023</span>
&lt;= J servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">12.011539</span></code></pre>
<p>On se rend compte que les performances de notre serveur de fast-food se dégradent : certains clients attendent jusqu'à trois fois plus longtemps que les autres.</p>
<p>Cela n'a rien de surprenant. En fait, les performances d'une application asynchrone ne se mesurent pas en <em>nombre de tâches traitées simultanément</em>, mais plutôt, comme n'importe quel serveur, en <em>nombre de tâches traitées dans le temps</em>. Il est évident que si 10 clients viennent manger dans un fast-food, il y a relativement peu de chances qu'ils arrivent tous en même temps : ils vont plutôt passer leur commande à raison d'une par seconde, par exemple.</p>
<p>Par contre, il est très important de noter que c'est bien <em>le temps d'attente</em> individuel de chaque client qui compte pour mesurer les performances (la qualité) du service. Si un client attend trop longtemps, il ne sera pas satisfait, peu importe s'il est tout seul dans le restaurant ou que celui-ci est bondé.</p>
<p>Pour ces raisons, il faut que nous ayons une idée des <strong>objectifs de performances</strong> de notre serveur, c'est-à-dire que nous fixions, comme but :</p>
<ul>
<li>un <em>temps d'attente maximal</em> à ne pas dépasser pour servir un client,</li>
<li>un <em>volume</em> de requêtes à tenir par seconde.</li>
</ul>
<p>Écrivons maintenant une coroutine pour tester les performances de notre serveur :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="co"># La fonction ensure_future est définie à partir de Python 3.4.4</span>
<span class="co"># Ce bloc la rend accessible pour toutes les versions de Python 3.4</span>
<span class="kw">try</span>:
    <span class="ch">from</span> asyncio <span class="ch">import</span> ensure_future
<span class="kw">except</span> <span class="ot">ImportError</span>:
    asyncio.ensure_future = asyncio.async

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> perf_test(nb_requests, period, timeout):
    tasks = []
    <span class="co"># On lance &#39;nb_requests&#39; commandes à &#39;period&#39; secondes d&#39;intervalle</span>
    <span class="kw">for</span> idx in <span class="dt">range</span>(<span class="dv">1</span>, nb_requests + <span class="dv">1</span>):
        client_name = <span class="st">&quot;client_{}&quot;</span>.<span class="dt">format</span>(idx)
        tsk = asyncio.ensure_future(serve(client_name))
        tasks.append(tsk)
        <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(period)

    finished, _ = <span class="kw">yield</span> <span class="ch">from</span> asyncio.wait(tasks)
    success = <span class="dt">set</span>()
    <span class="kw">for</span> tsk in finished:
        <span class="kw">if</span> tsk.result().seconds &lt; timeout:
            success.add(tsk)

    <span class="dt">print</span>(<span class="st">&quot;{}/{} clients satisfaits&quot;</span>.<span class="dt">format</span>(<span class="dt">len</span>(success), <span class="dt">len</span>(finished)))</code></pre>
<p>Cette coroutine va lancer un certain nombre de commandes, régulièrement, et compter à la fin le nombre de commandes qui ont été honorées dans les temps.</p>
<p>Essayons de lancer 10 commandes à 1 seconde d'intervalle, avec pour objectif que les clients soient servis en 5 secondes maximum :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop.run_until_complete(perf_test(<span class="dv">10</span>, <span class="dv">1</span>, <span class="dv">5</span>))
<span class="co"># ... sortie filtrée ...</span>
&lt;= client_1 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.004044</span>
&lt;= client_2 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">03.002792</span>
&lt;= client_3 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">03.003338</span>
&lt;= client_4 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">03.003653</span>
&lt;= client_5 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">03.003815</span>
&lt;= client_6 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.003746</span>
&lt;= client_7 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">03.003412</span>
&lt;= client_8 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">03.002512</span>
&lt;= client_9 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">03.003409</span>
&lt;= client_10 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">03.003622</span>
<span class="dv">10</span>/<span class="dv">10</span> clients satisfaits</code></pre>
<p>Ce test nous indique que notre serveur tient facilement une charge d'un client par seconde. Essayons de monter en charge en passant à deux clients par seconde :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop.run_until_complete(perf_test(<span class="dv">10</span>, <span class="fl">0.5</span>, <span class="dv">5</span>))
<span class="co"># ... sortie filtrée ...</span>
&lt;= client_1 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.002629</span>
&lt;= client_2 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">03.502093</span>
&lt;= client_3 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">03.002863</span>
&lt;= client_4 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.500168</span>
&lt;= client_5 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">04.500226</span>
&lt;= client_6 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">05.499894</span>
&lt;= client_7 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">05.999704</span>
&lt;= client_8 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">05.998824</span>
&lt;= client_9 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">05.999883</span>
&lt;= client_10 servi en <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">07.498776</span>
<span class="dv">5</span>/<span class="dv">10</span> clients satisfaits</code></pre>
<p>À deux clients par seconde, notre serveur n'offre plus de performances satisfaisantes pour la moitié des commandes.</p>
<p>Nous pouvons donc poser le problème d'optimisation suivant : le gérant du restaurant veut devenir capable de servir 2 clients par seconde avec un temps de traitement inférieur à 5 secondes par commande. Pour cela, il peut :</p>
<ul>
<li>Acheter de nouvelles machines à sodas ;</li>
<li>Embaucher de nouveaux cuisiniers ;</li>
<li>Remplacer son bac à frites (capable de cuire 5 portions en 4 secondes) par un nouveau, qui peut faire cuire 7 portions en 4 secondes.</li>
</ul>
<p>Évidemment, chacune de ces solutions a un coût, donc il est préférable pour le gérant de n'apporter que le moins possible de modifications pour tenir son objectif. Si l'on voulait faire un parallèle avec une application réelle :</p>
<ul>
<li>Acheter une seconde machine à sodas coûterait l'occupation à 100% d'un cœur de CPU supplémentaire + une augmentation de 100% de la RAM consommée par le service &quot;soda&quot;.</li>
<li>Embaucher un quatrième cuisinier coûterait un cœur de CPU supplémentaire + une augmentation de 33% de la RAM consommée par le service &quot;cuisine&quot;.</li>
<li>Le remplacement du bac à frites augmenterait uniquement de 40% la consommation de RAM de ce service…</li>
</ul>
<p>En guise d'exercice, vous pouvez vous amuser à modifier les contraintes de notre programme en conséquence pour observer l'impact de vos modifications sur les performances du serveur : vous vous trouverez alors véritablement dans la peau d'un architecte système, le salaire et le stress en moins. ;)</p>
<h1 id="exemple-n5-les-entréessorties-le-nerf-de-la-guerre"><span class="header-section-number">6</span> Exemple n°5 : Les entrées/sorties, le nerf de la guerre</h1>
<p>Vous vous demandez peut-être pourquoi on parle tout le temps d'<em>IO</em> en programmation asynchrone. En effet, les entrées/sorties des programmes semblent indissociables du concept d'asynchrone, à tel point que cela se traduit jusque dans le nom de la bibliothèque standard <code>asyncio</code> de Python. Mais <em>pourquoi</em> ?</p>
<p>Commençons par une définition : une <em>IO</em>, c'est une opération pendant laquelle un programme <em>interagit avec un flux de données</em>. Ce <strong>flux de données</strong> peut être plein de choses : une connexion réseau, les flux standard <code>STDIN</code>, <code>STDOUT</code> ou <code>STDERR</code> du processus en cours d'exécution, un fichier, ou même une abstraction matérielle<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>. « Interagir avec un flux de données », ça veut dire l'<strong>ouvrir</strong>, <strong>lire</strong> ou <strong>écrire</strong> dedans ou le <strong>fermer</strong>.</p>
<p>Jusqu'ici, nous avons travaillé sur des exemples très simples qui se contentaient d'afficher des choses à l'écran pour bien comprendre l'ordre dans lequel les instructions étaient exécutées. Nos tâches ne réalisaient du coup que des entrées/sorties, certes, mais celles-ci étaient <em>synchrones</em> : on a considéré jusqu'à maintenant qu'un <code>print()</code> dans la console s'exécute immédiatement et sans délai lors de son appel, ce qui est parfaitement intuitif...</p>
<p>... mais pas toujours le reflet de la réalité.</p>
<p>Prenons par exemple une IO très simple que vous réalisez en permanence sur votre ordinateur ou smartphone sans même vous en rendre compte : <strong>que se passe-t-il entre le moment où vous avez cliqué sur un lien dans une page web, et celui où le résultat commence à s'afficher sur votre écran</strong> ?</p>
<p>Eh bien vous <strong>attendez</strong>. Tout simplement. Et votre navigateur aussi. Sans rentrer dans le détail du protocole HTTP, on peut schématiser grossièrement ce qui se passe comme ceci :</p>
<pre><code>        Navigateur                           Serveur web
        ==========                           ===========

           [clic sur le lien]                     .
[création d&#39;une requête HTTP]                     .
       [connexion au serveur]  ------&gt;            .
            .                            [connexion reçue]
            .                  &lt;------   [connexion acceptée]
        [envoi de la requête]  ------&gt;            .
            .                            [réception de la requête]
            .                            [création de la réponse]
            .                  &lt;------   [envoi de la réponse]
    [réception de la réponse]                     .
       [affichage de la page]</code></pre>
<p>Dans ce schéma, tous les points (<code>.</code>) symbolisent une attente. Un échange HTTP (et plus généralement une IO), c'est une opération pendant laquelle les programmes, passent le plus clair de leur temps à <strong>ne rien faire</strong>. Et votre navigateur lui-même vous le dit (généralement dans un petit cadre en bas à gauche de l'écran) :</p>
<figure>
<img src="src/img/waiting.png" />
</figure>
<p>Dans ces conditions, l'idée de base de la programmation asynchrone est de <em>mettre à profit</em> tout ce temps que l'on passe à attendre pendant la réalisation d'une IO pour <strong>s'occuper en faisant autre chose</strong>.</p>
<p>L'exemple du serveur de <em>fast food</em> que nous avons modélisé plus tôt n'est pas anodin ; qu'il s'agisse du serveur <em>bien réel</em> d'un restaurant ou celui d'une application réseau, les deux réalisent en général des opérations comparables. En effet, de très nombreux serveurs (par exemple d'applications Web) fonctionnent plus ou moins suivant ce schéma :</p>
<ol type="1">
<li>Recevoir une requête, une commande ou un message,</li>
<li>Aller récupérer des ressources à différents endroits,</li>
<li>Combiner les ressources entre elles,</li>
<li>Répondre au client.</li>
</ol>
<p>Dans ce schéma, les points 1, 2 et 4 peuvent être des <em>IO</em> :</p>
<ol type="1">
<li>Réception :
<ul>
<li><strong>Attente</strong> d'une connexion,</li>
<li>Acceptation de la connexion,</li>
<li><strong>Attente</strong> du message,</li>
<li>Réception du message</li>
</ul></li>
<li>Récupérer des ressources :
<ul>
<li>S'il s'agit de ressources distantes :
<ul>
<li>Connexion à un service,</li>
<li><strong>Attente</strong> de l'acceptation de la connexion,</li>
<li>Envoi d'une requête,</li>
<li><strong>Attente</strong> que la réponse arrive,</li>
<li>Réception de la réponse,</li>
</ul></li>
<li>Si la ressource est protégée par un verrou ou un sémaphore :
<ul>
<li><strong>Attente</strong> de l'acquisition du verrou/sémaphore,</li>
<li>Récupération de la ressource,</li>
<li>Relâchement du verrou/sémaphore,</li>
</ul></li>
</ul></li>
<li>Combiner les ressources entre elles,</li>
<li>Répondre au client :
<ul>
<li><strong>Attente</strong> que le <em>medium</em> soit disponible en écriture,</li>
<li>Envoi de la réponse.</li>
</ul></li>
</ol>
<p>Comme vous le voyez, il est vraiment <em>très</em> courant d'attendre pour un serveur. Et il ne s'agit là que d'un schéma particulier dans une infinité d'applications possibles.</p>
<p>Ainsi, il serait possible d'optimiser de nombreux programmes en les rendant asynchrones, pour peu que l'on soit capable de rendre leurs <em>IO</em> <strong>non bloquantes</strong>, c'est-à-dire que l'on puisse laisser la main à d'autres tâches au lieu d'attendre, et reprendre celle-ci avec l'assurance que l'on pourra réaliser une <em>IO</em> immédiatement.</p>
<p>Il existe sous la plupart des systèmes d'exploitation une fonctionnalité qui permet de déterminer à un instant donné si un ou plusieurs flux de données sont accessibles en lecture ou en écriture. En fait, il en existe plein, mais nous allons nous concentrer sur celle qui sera disponible sur la plupart des systèmes d'exploitation : il s'agit de l'appel-système <a href="https://docs.python.org/3.4/library/select.html#select.select"><code>select()</code></a>.</p>
<p>Celui-ci, en Python, se présente sous la forme suivante :</p>
<pre class="sourceCode python"><code class="sourceCode python">select.select(rlist, wlist, xlist[, timeout])</code></pre>
<p>Où :</p>
<ul>
<li><code>rlist</code> est une liste de flux sur lesquels nous voulons lire des données,</li>
<li><code>wlist</code> est une liste de flux dans lesquels nous voulons écrire des données,</li>
<li><code>xlist</code> est une liste de flux que l'on surveille jusqu'à ce qu'il se produise des <em>conditions exceptionnelles</em> (ne nous attardons pas là-dessus),</li>
<li><code>timeout</code> est une durée (optionnelle) pendant laquelle on attend que des flux soient disponibles. Par défaut, on attend indéfiniment. Si on lui passe la valeur <code>0</code>, l'appel à <code>select()</code> retourne immédiatement.</li>
</ul>
<p>Cette fonction retourne trois listes :</p>
<ul>
<li>une contenant les flux disponibles en lecture à l'instant T,</li>
<li>une contenant les flux disponibles en écriture à l'instant T,</li>
<li>une autre contenant les flux victimes d'une exception.</li>
</ul>
<p>Que demander de plus ? Nous avons à notre disposition une fonction, dont l'exécution est instantannée, qui pourra nous permettre de réveiller les tâches en attente de lecture ou d'écriture.</p>
<p>On peut donc implémenter les coroutines d'attente asynchrones suivantes :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> select <span class="ch">import</span> select

<span class="co"># Attendre de façon asynchrone qu&#39;un flux soit disponible en lecture</span>
<span class="kw">def</span> wait_readable(stream):
    <span class="kw">while</span> <span class="ot">True</span>:
        rlist, _, _ = select([stream], [], [], <span class="dv">0</span>)
        <span class="kw">if</span> rlist:
            <span class="kw">return</span> stream
        <span class="kw">yield</span>

<span class="co"># Attendre de façon asynchrone qu&#39;un flux soit disponible en écriture</span>
<span class="kw">def</span> wait_writable(stream):
    <span class="kw">while</span> <span class="ot">True</span>:
        _, wlist, _ = select([], [stream], [], <span class="dv">0</span>)
        <span class="kw">if</span> wlist:
            <span class="kw">return</span> stream
        <span class="kw">yield</span></code></pre>
<p><strong>Note:</strong> Sous la plupart des systèmes d'exploitation Unix, vous pouvez utiliser <code>select()</code> pour attendre après n'importe quel flux de données (flux standard, fichiers, sockets), mais sous Windows, <em>seules</em> les sockets sont supportées.</p>
<p>Pour bien comprendre l'apport de ces deux fonctions, commençons par écrire un petit serveur qui se contente d'attendre une seconde avant de renvoyer les messages des clients :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> socket <span class="ch">import</span> socket
<span class="ch">from</span> time <span class="ch">import</span> sleep

<span class="kw">def</span> echo_server():
    <span class="co"># On crée une socket (par défaut : TCP/IP)</span>
    <span class="co"># qui écoutera sur le port 1234</span>
    sock = socket()
    sock.bind((<span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>))

    <span class="co"># On garde un maximum de 5 demandes de connexion en attente</span>
    sock.listen(<span class="dv">5</span>)
    <span class="kw">try</span>:
        <span class="kw">while</span> <span class="ot">True</span>:
            <span class="co"># Acceptation de la connection</span>
            conn, host = sock.accept()

            <span class="co"># Réception d&#39;un message (4 Mio max.)</span>
            msg = conn.recv(<span class="dv">4096</span>)
            <span class="dt">print</span>(<span class="st">&quot;message reçu de {!r}: {}&quot;</span>.<span class="dt">format</span>(host, msg.decode()))
            sleep(<span class="dv">1</span>)

            <span class="co"># Renvoi du message</span>
            conn.send(msg)
            conn.close()
    <span class="kw">finally</span>:
        sock.close()</code></pre>
<p>Lançons ce serveur dans une console.</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; echo_server()</code></pre>
<p>Dans <strong>une autre console</strong>, nous pouvons vérifier que celui-ci fonctionne en lui envoyant un message.</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; sock = socket.socket()
&gt;&gt;&gt; sock.<span class="ot">connect</span>((<span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>))
&gt;&gt;&gt; sock.send(<span class="st">&quot;Ohé !&quot;</span>.encode())
<span class="dv">6</span></code></pre>
<p>Le serveur affiche alors :</p>
<pre><code>message reçu de (&#39;127.0.0.1&#39;, 40568): Ohé !</code></pre>
<p>Nous n'avons plus qu'à récupérer sa réponse dans la fenêtre du client :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; data = sock.recv(<span class="dv">1024</span>)
&gt;&gt;&gt; data.decode()
<span class="co">&#39;Ohé !&#39;</span></code></pre>
<p>Parfait.</p>
<p>Pour corser les choses, essayons maintenant de lui envoyer 5 messages à la fois. Utilisons pour cela <code>asyncio</code>, ainsi que les deux coroutines d'attente que nous venons d'écrire :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> socket <span class="ch">import</span> socket
<span class="ch">from</span> datetime <span class="ch">import</span> datetime

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> echo(msg):
    <span class="co"># Connection TCP au port 1234</span>
    sock = socket()
    sock.<span class="ot">connect</span>((<span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>))

    <span class="co"># On attend de pouvoir envoyer le message</span>
    <span class="kw">yield</span> <span class="ch">from</span> wait_writable(sock)
    <span class="dt">print</span>(<span class="st">&quot;Envoi du message {!r}&quot;</span>.<span class="dt">format</span>(msg))
    sock.send(msg.encode())

    <span class="co"># On attend la réponse</span>
    <span class="kw">yield</span> <span class="ch">from</span> wait_readable(sock)
    data = sock.recv(<span class="dv">4096</span>)
    <span class="dt">print</span>(<span class="st">&quot;Message reçu:&quot;</span>, data.decode())
    sock.close()

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> echo_client():
    start = datetime.now()
    tasks = [echo(<span class="st">&quot;Hello {}&quot;</span>.<span class="dt">format</span>(num)) <span class="kw">for</span> num in <span class="dt">range</span>(<span class="dv">5</span>)]
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.wait(tasks)
    <span class="dt">print</span>(<span class="st">&quot;temps total:&quot;</span>, datetime.now() - start)</code></pre>
<p>Rien de fondamentalement compliqué : la coroutine <code>echo()</code> se contente de faire ce que nous avons réalisé juste avant dans la console, mais en attendant de façon asynchrone que la socket soit disponible en écriture, puis en lecture.</p>
<p>Voici le résultat :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(echo_client())
Envoi du message <span class="st">&#39;Hello 2&#39;</span>
Envoi du message <span class="st">&#39;Hello 1&#39;</span>
Envoi du message <span class="st">&#39;Hello 0&#39;</span>
Envoi du message <span class="st">&#39;Hello 3&#39;</span>
Envoi du message <span class="st">&#39;Hello 4&#39;</span>
Message reçu: Hello <span class="dv">2</span>
Message reçu: Hello <span class="dv">1</span>
Message reçu: Hello <span class="dv">0</span>
Message reçu: Hello <span class="dv">3</span>
Message reçu: Hello <span class="dv">4</span>
temps total: <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">05.013461</span></code></pre>
<p>À l'exécution, nous nous rendons compte que le serveur, qui est <em>synchrone</em>, traite les messages les uns à la suite des autres. Réimplémentons-le avec <code>asyncio</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> socket <span class="ch">import</span> socket
<span class="ch">import</span> asyncio

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> serve_echo(conn, host):
    <span class="co"># On suspend l&#39;exécution jusqu&#39;à recevoir un message</span>
    <span class="kw">yield</span> <span class="ch">from</span> wait_readable(conn)
    msg = conn.recv(<span class="dv">4096</span>)
    <span class="dt">print</span>(<span class="st">&quot;message reçu de {!r}: {}&quot;</span>.<span class="dt">format</span>(host, msg.decode()))
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">1</span>)

    <span class="co"># On suspend l&#39;exécution jusqu&#39;à pouvoir répondre au client</span>
    <span class="kw">yield</span> <span class="ch">from</span> wait_writable(conn)
    conn.send(msg)
    conn.close()

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> echo_server():
    <span class="co"># On crée une socket (par défaut : TCP/IP)</span>
    <span class="co"># qui écoutera sur le port 1234</span>
    sock = socket()
    sock.bind((<span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>))

    <span class="co"># On garde un maximum de 5 demandes de connexion en attente</span>
    sock.listen(<span class="dv">5</span>)
    <span class="kw">try</span>:
        <span class="kw">while</span> <span class="ot">True</span>:
            <span class="co"># On attent qu&#39;une demande de connexion arrive</span>
            <span class="kw">yield</span> <span class="ch">from</span> wait_readable(sock)

            <span class="co"># Acceptation de la connection</span>
            conn, host = sock.accept()

            <span class="co"># On programme le traitement de la requête dans une tâche séparée.</span>
            asyncio.async(serve_echo(conn, host))
    <span class="kw">finally</span>:
        sock.close()</code></pre>
<p>Lançons le serveur :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(echo_server())</code></pre>
<p>Puis le client :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; loop.run_until_complete(echo_client())
Envoi du message <span class="st">&#39;Hello 3&#39;</span>
Envoi du message <span class="st">&#39;Hello 0&#39;</span>
Envoi du message <span class="st">&#39;Hello 2&#39;</span>
Envoi du message <span class="st">&#39;Hello 1&#39;</span>
Envoi du message <span class="st">&#39;Hello 4&#39;</span>
Message reçu: Hello <span class="dv">3</span>
Message reçu: Hello <span class="dv">0</span>
Message reçu: Hello <span class="dv">2</span>
Message reçu: Hello <span class="dv">1</span>
Message reçu: Hello <span class="dv">4</span>
temps total: <span class="dv">0</span>:<span class="dv">00</span>:<span class="fl">01.001991</span></code></pre>
<p>Cette fois-ci, le serveur a traité toutes les requêtes en même temps, pour un temps d'exécution total de 1s au lieu de 5s.</p>
<p>Nous voici maintenant capables de réaliser des <em>IO</em> asynchrones ! Néanmoins, bien que nos coroutines <code>wait_readable()</code> et <code>wait_writable()</code> soient tout à fait fonctionnelles dans cet exemple, je vous <strong>déconseille très vivement</strong> de les utiliser. En effet, celles-ci bouclent à l'infini jusqu'à ce que le flux soit disponible, au lieu d'attendre passivement sans consommer de temps sur le processeur.</p>
<p>Rassurez-vous, <code>asyncio</code> propose de nombreuses façons de réaliser la même chose, et ce sans surcharger le processeur. Nous les examinerons dans les exemples suivants.</p>
<h1 id="exemple-n6-des-applications-réseau-avec-asyncio"><span class="header-section-number">7</span> Exemple n°6 : Des applications réseau avec <code>asyncio</code></h1>
<p>Le précédent exemple nous a fait toucher du doigt le concept d'<em>IO asynchrones</em>. Il est temps pour nous de nous familiariser avec les objets que nous propose <code>asyncio</code> pour réaliser de telles opérations sur un flux réseau.</p>
<p>Commençons par examiner un exemple que nous détaillerons ensuite. Voici le client du précédent exemple, réimplémenté avec les outils d'<code>asyncio</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> echo_client(message):
    reader, writer = <span class="kw">yield</span> <span class="ch">from</span> asyncio.open_connection(<span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>)

    <span class="dt">print</span>(<span class="st">&quot;Envoi du message :&quot;</span>, message)
    writer.write(message.encode())

    data = <span class="kw">yield</span> <span class="ch">from</span> reader.read(<span class="dv">1024</span>)
    <span class="dt">print</span>(<span class="st">&quot;Message reçu :&quot;</span>, data.decode())

    writer.close()</code></pre>
<p>Même sans savoir ce que sont ces objets <code>reader</code> et <code>writer</code>, la lecture de ce code est plutôt intuitive :</p>
<ul>
<li>La coroutine <code>ayncio.open_connection()</code> crée une connexion et retourne deux objets :
<ul>
<li>Un &quot;<em>reader</em>&quot; que l'on peut utiliser pour recevoir des données,</li>
<li>Un &quot;<em>writer</em>&quot; dont on se sert pour en envoyer,</li>
</ul></li>
<li>On se sert du <code>writer</code> pour envoyer un message au serveur (attention : la méthode <code>write()</code> est une fonction et non une coroutine ; pas de <code>yield from</code>),</li>
<li>On appelle ensuite, cette fois de façon asynchrone, la méthode <code>reader.read()</code> pour récupérer des données (on s'attend à moins de 1024 octets),</li>
<li>On ferme le <code>writer</code> pour clore la connexion.</li>
</ul>
<p>Avant d'aller plus loin, lançons le serveur de l'exemple précédent et vérifions que ce code fonctionne dans la console :</p>
<pre class="sourceCode python"><code class="sourceCode python">&gt;&gt;&gt; <span class="ch">import</span> asyncio
&gt;&gt;&gt; loop = asyncio.get_event_loop()
&gt;&gt;&gt; loop.run_until_complete(echo_client(<span class="st">&quot;Coucou!&quot;</span>))
Envoi du message : Coucou!
Message reçu : Coucou!</code></pre>
<p>Par de surprise, le code se comporte comme prévu. Attardons-nous un peu sur ces fameux objets qu'<code>asyncio</code> a créés pour nous. Il s'agit en fait d'instances de <a href="https://docs.python.org/3/library/asyncio-stream.html#streamreader"><code>asyncio.StreamReader</code></a> et <a href="https://docs.python.org/3/library/asyncio-stream.html#streamwriter"><code>asyncio.StreamWriter</code></a>.</p>
<p>Il s'agit de l'interface haut niveau d'asyncio pour manipuler une connexion de type &quot;<em>Stream</em>&quot; (typiquement, TCP). La même interface existe également pour les sockets Unix. Ces objets représentent un les deux sens d'un même flux réseau. Ainsi, vous avez grosso-modo 4 méthodes à retenir :</p>
<ul>
<li>La <em>coroutine</em> <code>StreamReader.read(size)</code> sert à recevoir un bloc de <code>size</code> octets de données maximum, de façon asynchrone ;</li>
<li>La <em>coroutine</em> <code>StreamReader.readline()</code> sert à recevoir une &quot;ligne&quot; de données, terminée par le caractère ASCII spécial <em>LINE_FEED</em> (<code>'\n'</code>) ;</li>
<li>La <em>fonction</em> <code>StreamWriter.write(data)</code> sert à envoyer des données (sous la forme d'un objet <code>bytes</code>) sur le flux. En fait, les données seront placées en attente dans un tampon (<em>buffer</em>) qui ne sera vidé qu'au prochain <code>yield</code>…</li>
<li>Ce qui nous amène à la <em>coroutine</em> <code>StreamWriter.drain()</code>, qui sert à vider explicitement le tampon du <em>writer</em> pour envoyer les données sur le flux.</li>
</ul>
<p>C'est plutôt simple, non ? Ces objets reposent en fait sur une autre API, un petit peu plus bas niveau, d'<code>asyncio</code>, qui modélise des <em>protocoles</em> que l'on utilise par-dessus un <em>transport</em> (TCP, UDP…). Nous ne parlerons pas de cette API dans cet exemple, mais si vous êtes curieux, <a href="https://docs.python.org/3/library/asyncio-protocol.html">sa documentation</a> est plutôt claire et détaillée.</p>
<p>Cette même interface peut d'ailleurs nous permettre de créer facilement un serveur, au moyen de la coroutine <code>asyncio.start_server()</code> :</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="co">#!/usr/bin/env python3</span>
<span class="ch">import</span> asyncio

<span class="ot">@asyncio.coroutine</span>
<span class="kw">def</span> handle_echo(reader, writer):
    data = <span class="kw">yield</span> <span class="ch">from</span> reader.read(<span class="dv">1024</span>)
    <span class="dt">print</span>(<span class="st">&quot;Message reçu :&quot;</span>, data.decode())
    <span class="kw">yield</span> <span class="ch">from</span> asyncio.sleep(<span class="dv">1</span>)
    writer.write(data)
    <span class="kw">yield</span> <span class="ch">from</span> writer.drain()
    writer.close()

<span class="kw">def</span> echo_server():
    loop = asyncio.get_event_loop()
    server = loop.run_until_complete(
        asyncio.start_server(handle_echo, <span class="st">&#39;localhost&#39;</span>, <span class="dv">1234</span>)
    )

    <span class="kw">try</span>:
        loop.run_forever()
    <span class="kw">except</span> <span class="ot">KeyboardInterrupt</span>:
        <span class="kw">pass</span>

    server.close()
    loop.run_until_complete(server.wait_closed())
    loop.close()

<span class="kw">if</span> <span class="ot">__name__</span> == <span class="st">&#39;__main__&#39;</span>:
    echo_server()</code></pre>
<p>Ici, tout le code de notre serveur se trouve dans la coroutine <code>handle_echo</code>. Cette coroutine accepte un <code>StreamReader</code> et un <code>StreamWriter</code>, et ne retourne que lorsque l'échange avec un client est terminé.</p>
<p>Dans la fonction <code>echo_server()</code> on commence par créer un serveur en passant cette coroutine à <code>asyncio.start_server()</code> dans la boucle événementielle, puis on demande à la boucle de tourner &quot;à l'infini&quot; (jusqu'à ce que le processus soit interrompu).</p>
<p>Notez que nous aurions pu instancier le même service, mais sur des sockets Unix au lieu d'un flux TCP, simplement en utilisant la fonction <code>asyncio.start_unix_server()</code> à la place de <code>asyncio.start_server()</code> : la coroutine <code>handle_echo</code>, restera la même, et sera lancée dans une nouvelle tâche chaque fois qu'un client se connectera.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>En toute rigueur il s'agit d'un <em>générateur</em>, mais comme nous avons pu l'observer dans un <a href="https://zestedesavoir.com/articles/232/la-puissance-cachee-des-coroutines/">précédent article</a>, les générateurs de Python sont implémentés comme de véritables coroutines.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>D'ailleurs, le noyau de l'OS est justement là pour vous empêcher de toucher vous-même à l'ordonnanceur !<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Un <em>processus</em> peut être vu comme <strong>un programme en cours d'exécution</strong>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Ce comportement n'est pas exclusif aux processus : de la même façon, un <em>thread</em> <code>A</code> envoie au noyau un appel système lorsqu'il veut lancer un nouveau <em>thread</em> <code>B</code>.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>On peut par exemple lire un son sous Linux en <em>écrivant</em> des données dans un fichier spécial qui représente la carte son !<a href="#fnref5">↩</a></p></li>
</ol>
</section>
</body>
</html>
